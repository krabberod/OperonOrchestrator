{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krabberod/OperonOrchestrator/blob/main/vsearch_swarm_pipeline_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g0avc0nUYudy",
      "metadata": {
        "id": "g0avc0nUYudy"
      },
      "source": [
        "Metabarcoding Pipeline\n",
        "======================\n",
        "\n",
        "Original script by Frédéric Mahé, April 19th 2023,  \n",
        "Slightly modified and updated by Anders K. Krabberød and Ramiro April 2025.\n",
        "\n",
        "Contrary to traditional pipelines, most filtering steps in this pipeline are done after the clustering/denoising step, when the risk of eliminating real molecular diversity is minimized.\n",
        "\n",
        "*Please feel free to ask questions at anytime during this hands-on\n",
        "session.*\n",
        "\n",
        "Part 0: crash-course and set-up\n",
        "-------------------------------\n",
        "\n",
        "A quick introduction on the type of code you will see today.\n",
        "\n",
        "### shell\n",
        "\n",
        "#### code block\n",
        "\n",
        "The pipeline is made of blocks of shell code, written in bash: Bourne Again SHell. It's one of the most commonly used Unix/Linux command-line shells and scripting languages. Bash lets users interact with the system, run commands, and automate tasks via scripts.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "``` shell\n",
        "# variables\n",
        "THREADS=4\n",
        "ENCODING=33\n",
        "\n",
        "# some comments\n",
        "vsearch \\\n",
        "    --threads ${THREADS} \\\n",
        "    --fastq_mergepairs R1.fastq.gz \\\n",
        "    --reverse R2.fastq.gz \\\n",
        "    --fastq_ascii ${ENCODING} \\\n",
        "    --fastq_allowmergestagger \\\n",
        "    --quiet \\\n",
        "    --fastqout out.fastq\n",
        "```\n",
        "\n",
        "Some blocks can be executed in Google Colab. Blocks that can't be\n",
        "executed are examples, like the one above. They are not necessary for\n",
        "the pipeline.\n",
        "\n",
        "**Redirecting**\n",
        "It's possible to redirect a shell command's output with the greater-than symbol symbol \">\". Beware, this will overwrite any existing file.\n",
        "\n",
        "``` shell\n",
        "# basics\n",
        "command > output.fastq\n",
        "command 2> output.log\n",
        "command 2> /dev/null\n",
        "command < input.fastq\n",
        "```\n",
        "\n",
        "**Line Wrapping:**\n",
        "A line can be wrapped with if it gets too long to read.\n",
        "A matter of personal preference:\n",
        "\n",
        "``` shell\n",
        "# too long to read:\n",
        "vsearch --threads 4 --fastq_mergepairs R1.fastq.gz --reverse R2.fastq.gz --fastq_ascii 33 --fastq_allowmergestagger --quiet --fastqout out.fastq\n",
        "\n",
        "# wrapping makes it more readable:\n",
        "vsearch \\\n",
        "    --threads 4 \\\n",
        "    --fastq_mergepairs R1.fastq.gz \\\n",
        "    --reverse R2.fastq.gz \\\n",
        "    --fastq_ascii 33 \\\n",
        "    --fastq_allowmergestagger \\\n",
        "    --quiet \\\n",
        "    --fastqout out.fastq\n",
        "```\n",
        "\n",
        "**The Pipe symbol**  \n",
        "This is the origin of the word *pipeline* —a sequence of commands executed in series, passing data from one to the next. It’s a powerful feature of Unix/Linux systems.\n",
        "\n",
        "``` shell\n",
        "# slow\n",
        "command1 input.fastq > tmp1.fastq\n",
        "command2 tmp1.fastq > tmp2.fastq\n",
        "command3 tmp2.fastq > final_output.fastq\n",
        "\n",
        "# use pipes to avoid temporary files:\n",
        "command1 input.fastq | \\\n",
        "    command2 | \\\n",
        "    command3 > final_output.fastq\n",
        "```\n",
        "\n",
        "Note: pipes are so useful that they were recently added to R, C++, and\n",
        "other languages.\n",
        "\n",
        "**Tee**\n",
        "\n",
        "Use a `tee` to save an intermediary result:\n",
        "\n",
        "``` shell\n",
        "command1 input.fastq | \\\n",
        "    command2 | \\\n",
        "    tee output2.fastq | \\\n",
        "    command3 > final_output.fastq\n",
        "```\n",
        "\n",
        "A `tee` will duplicate your stream of data, allowing you to do process\n",
        "it in two different ways simultaneously. It is possible to connect\n",
        "multiple `tee`s and to create a multi-furcation.\n",
        "\n",
        "#### test\n",
        "\n",
        "The shell is a great tool to manipulate text. You can easily create fake\n",
        "data and pass it to a software you would like to test:\n",
        "\n",
        "``` shell\n",
        "# create toy-examples:\n",
        "printf \">s_1\\nA\\n\"\n",
        "\n",
        "# use them to test software behavior:\n",
        "printf \">s_1\\nA\\n\" | \\\n",
        "    swarm\n",
        "```\n",
        "\n",
        "Note: documentation rarely is 100% complete, when you have a doubt about\n",
        "a tool, create a toy-example to test its behavior. We’ll dive deeper\n",
        "into code testing this afternoon during the `lulu` session.\n",
        "\n",
        "### FASTQ format\n",
        "\n",
        "Metabarcoding data are usually available in\n",
        "[FASTQ](https://en.wikipedia.org/wiki/FASTQ_format#Encoding) format:\n",
        "\n",
        "-   most-frequent format,\n",
        "-   human-readable,\n",
        "-   encode quality values (probability of error for each position),\n",
        "-   can be hard to parse,\n",
        "-   encoding type must be guessed\n",
        "\n",
        "<!-- -->\n",
        "\n",
        "    @M05074:97:000000000-BPW9G:1:1101:10203:1383 1:N:0:2\n",
        "    CATAATTTCCTCCGCTTATTGATATGCTTAAGTTCAGCGGGTATCCCTACCTGATCCGAGTTCAACCTAAGAAAGTTGGGGGTTCTGGCGGGTGGACGGCTGAACCCTGTAGCGACAAGTATTACTACGCTTAGAGCCAGACGGCACCGCCACTGCTTTTAAGTGCCGCCGGTACAGCGGGCCCCAAGGCCAAGCAGAGCTTGATTGGTCA\n",
        "    +\n",
        "    @-A-9EFGFFFFD7BFF7FE9,C9F<EFG99,CEF9,@77+@+CCC@F9FCF9,C@C,,+,8C9<CEF,,,,,,,CF,,+++8FEF9,?+++@+++B++@C+,,B?FE8E,,<+++++3C,CF9DF9>>CFE7,,3=@7,,@++@:FC7BC*CC:,7>DF9,,,,7?*=B*5?*:++7***=?EE3***2;***:*0*/;@C8*<C+*<<+\n",
        "\n",
        "Note: the quality line may starts with a @ …\n",
        "\n",
        "Note: Q values are a way to encode on one character numerical values\n",
        "ranging from 0 to 41 (usually). These values represent the probability\n",
        "of a wrong base calling for that particular position. Q20 means 1% of\n",
        "risk, Q30 means 0.1%, and Q40 means 0.01%.\n",
        "\n",
        "In paired-end sequencing, there are two files per sample: R1 and R2.\n",
        "Each R1 read has a R2 counterpart, and reads are in the same order in\n",
        "both files.\n",
        "\n",
        "### Google Colab\n",
        "\n",
        "Let’s explore this environment. This is our first executable block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HU5Zbp39Yudz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU5Zbp39Yudz",
        "outputId": "6cbc7641-46f0-4209-b8fb-39bace1939d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Apr  5 09:08:21 PM UTC 2025\n",
            "root\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "date\n",
        "whoami"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdufhIGrYudz",
      "metadata": {
        "id": "bdufhIGrYudz"
      },
      "source": [
        "We are `root`! Maximal clearance level, we can do anything we want.\n",
        "\n",
        "Check the OS version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PO92rryTYudz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO92rryTYudz",
        "outputId": "297cbcc4-41cd-4422-d655-5dc0fc17ddb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.4 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n",
            "Linux b4bc36502872 6.1.85+ #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "lsb_release -a\n",
        "uname -a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xvpz8UosYudz",
      "metadata": {
        "id": "Xvpz8UosYudz"
      },
      "source": [
        "The operating system is Ubuntu 22.04.4 LTS. LTS stands for long-term\n",
        "support.\n",
        "Check basic utilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tx4JHl0kYudz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx4JHl0kYudz",
        "outputId": "662547ae-6ecf-4e9e-9725-be5c0ec3eec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNU bash, version 5.1.16(1)-release (x86_64-pc-linux-gnu)\n",
            "Copyright (C) 2020 Free Software Foundation, Inc.\n",
            "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
            "\n",
            "This is free software; you are free to change and redistribute it.\n",
            "There is NO WARRANTY, to the extent permitted by law.\n",
            "git version 2.34.1\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "Python 3.11.11\n",
            "R version 4.4.3 (2025-02-28) -- \"Trophy Case\"\n",
            "Copyright (C) 2025 The R Foundation for Statistical Computing\n",
            "Platform: x86_64-pc-linux-gnu\n",
            "\n",
            "R is free software and comes with ABSOLUTELY NO WARRANTY.\n",
            "You are welcome to redistribute it under the terms of the\n",
            "GNU General Public License versions 2 or 3.\n",
            "For more information about these matters see\n",
            "https://www.gnu.org/licenses/.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "bash --version\n",
        "git --version\n",
        "gcc --version\n",
        "python --version\n",
        "R --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zk4QAkeAYudz",
      "metadata": {
        "id": "Zk4QAkeAYudz"
      },
      "source": [
        "`R`, `git` and the compilation tool-chain are already installed.\n",
        "\n",
        "The `gcc` version is a bit old (9.2), and might not allow to compile\n",
        "code based on very recent standards (e.g.; `mumu` which uses C++20\n",
        "features).\n",
        "\n",
        "What about hardware resources?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n8vGN5x6Yudz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8vGN5x6Yudz",
        "outputId": "e64adbc6-bca9-4ba3-cf86-f4c8166f766d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   37G   71G  35% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  820M  59% /usr/sbin/docker-init\n",
            "tmpfs           6.4G  324K  6.4G   1% /var/colab\n",
            "/dev/sda1        85G   66G   20G  78% /kaggle/input\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "MemTotal:       13290460 kB\n",
            "MemFree:         7936036 kB\n",
            "MemAvailable:   12242200 kB\n",
            "Buffers:          401488 kB\n",
            "Cached:          4078604 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           712276 kB\n",
            "Inactive:        4383436 kB\n",
            "Active(anon):       1200 kB\n",
            "Inactive(anon):   616264 kB\n",
            "Active(file):     711076 kB\n",
            "Inactive(file):  3767172 kB\n",
            "Unevictable:           8 kB\n",
            "Mlocked:               8 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:             20724 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        615696 kB\n",
            "Mapped:           316556 kB\n",
            "Shmem:              1824 kB\n",
            "KReclaimable:     137708 kB\n",
            "Slab:             176516 kB\n",
            "SReclaimable:     137708 kB\n",
            "SUnreclaim:        38808 kB\n",
            "KernelStack:        5284 kB\n",
            "PageTables:        10988 kB\n",
            "SecPageTables:         0 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6645228 kB\n",
            "Committed_AS:    2589700 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       11608 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1144 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "Unaccepted:            0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       60216 kB\n",
            "DirectMap2M:     5179392 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "df -h\n",
        "cat /proc/cpuinfo\n",
        "cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uXe6sCxDYudz",
      "metadata": {
        "id": "uXe6sCxDYudz"
      },
      "source": [
        "It seems that Google colab instances are virtual x86-64 machines with 2\n",
        "CPU-cores (Intel Xeon @ 2.20GHz), 12 GB of RAM and 85 GB of storage\n",
        "space.\n",
        "\n",
        "I will assume that you also have [python](https://www.python.org/)\n",
        "(version 3.7 or more), [R](https://cran.r-project.org/) (version 4.0 or\n",
        "more), and [bash](https://www.gnu.org/software/bash/) (version 4 or\n",
        "more).\n",
        "\n",
        "Last check : is it possible to pass data from one `shell` code block to\n",
        "another?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fRmp0-dYudz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fRmp0-dYudz",
        "outputId": "65a11542-90ed-4493-ebdb-a99c7ded294f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/tmp\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "pwd\n",
        "mkdir -p tmp\n",
        "cd ./tmp/\n",
        "pwd\n",
        "i=5\n",
        "export j=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JX1_qi1uYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX1_qi1uYud0",
        "outputId": "1bedd459-ea81-457f-862c-5059b2a3cb15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "i=\n",
            "j=\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "pwd\n",
        "echo \"i=\"$i\n",
        "echo \"j=\"$j"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kRaWAsuvYud0",
      "metadata": {
        "id": "kRaWAsuvYud0"
      },
      "source": [
        "It seems that in `%%shell` code blocks, `cd` moves, variable\n",
        "declarations, and function declarations are limited to the current block\n",
        "(no effect on downstream code blocks). Only file and folder creations\n",
        "are persistent.\n",
        "\n",
        "### install dependencies\n",
        "\n",
        "Let’s create some folders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zvx3OKJqYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvx3OKJqYud0",
        "outputId": "4183c2df-121e-4171-b7e6-d9c85c32a285"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p src data references results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ptQyLzaQit",
      "metadata": {
        "id": "22ptQyLzaQit"
      },
      "source": [
        "Check the where you are in the folder structure and list the content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FOnpi511aQGb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOnpi511aQGb",
        "outputId": "0e19333a-45d9-4a31-adef-7fc0d70fb6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "total 24K\n",
            "drwxr-xr-x 2 root root 4.0K Apr  5 21:09 data\n",
            "drwxr-xr-x 2 root root 4.0K Apr  5 21:09 references\n",
            "drwxr-xr-x 2 root root 4.0K Apr  5 21:09 results\n",
            "drwxr-xr-x 1 root root 4.0K Apr  3 13:37 sample_data\n",
            "drwxr-xr-x 2 root root 4.0K Apr  5 21:09 src\n",
            "drwxr-xr-x 2 root root 4.0K Apr  5 21:09 tmp\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "pwd\n",
        "ls -lh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LSnsm6UPamUQ",
      "metadata": {
        "id": "LSnsm6UPamUQ"
      },
      "source": [
        "Current location is in in the folder `/content/`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Z0UGd_EYud0",
      "metadata": {
        "id": "4Z0UGd_EYud0"
      },
      "source": [
        "We will need to install [vsearch](https://github.com/torognes/vsearch),\n",
        "[cutadapt](https://github.com/marcelm/cutadapt/), and\n",
        "[swarm](https://github.com/torognes/swarm).\n",
        "\n",
        "#### install cutadapt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA69ZmVrYud0",
      "metadata": {
        "id": "fA69ZmVrYud0"
      },
      "source": [
        "Let's install `cutadapt` with `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ymalj9yPYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymalj9yPYud0",
        "outputId": "c12c14f1-5bcb-4bda-920b-d09d1199b176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.0.1\n",
            "Collecting cutadapt\n",
            "  Downloading cutadapt-5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting dnaio>=1.2.3 (from cutadapt)\n",
            "  Downloading dnaio-1.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting xopen>=1.6.0 (from cutadapt)\n",
            "  Downloading xopen-2.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting isal>=1.6.1 (from xopen>=1.6.0->cutadapt)\n",
            "  Downloading isal-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting zlib-ng>=0.4.1 (from xopen>=1.6.0->cutadapt)\n",
            "  Downloading zlib_ng-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Downloading cutadapt-5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "Downloading dnaio-1.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (108 kB)\n",
            "Downloading xopen-2.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading isal-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (260 kB)\n",
            "Downloading zlib_ng-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (108 kB)\n",
            "Installing collected packages: zlib-ng, isal, xopen, dnaio, cutadapt\n",
            "Successfully installed cutadapt-5.0 dnaio-1.2.3 isal-1.7.2 xopen-2.0.2 zlib-ng-0.5.1\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "python3 -m pip install --upgrade pip\n",
        "python3 -m pip install --upgrade cutadapt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nq2rhuxpYud0",
      "metadata": {
        "id": "Nq2rhuxpYud0"
      },
      "source": [
        "check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P4QP5lTuYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4QP5lTuYud0",
        "outputId": "af87a062-976e-4b74-a7b5-d3aa3cf3a8d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cutadapt --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vb5mWYuAYud0",
      "metadata": {
        "id": "vb5mWYuAYud0"
      },
      "source": [
        "Let's install additional tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8SdIEt1PYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SdIEt1PYud0",
        "outputId": "db0c2168-505c-45f7-c048-0cc4d8487ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  dos2unix\n",
            "0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 384 kB of archives.\n",
            "After this operation, 1,367 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dos2unix amd64 7.4.2-2 [384 kB]\n",
            "Fetched 384 kB in 0s (1,425 kB/s)\n",
            "Selecting previously unselected package dos2unix.\n",
            "(Reading database ... 126213 files and directories currently installed.)\n",
            "Preparing to unpack .../dos2unix_7.4.2-2_amd64.deb ...\n",
            "Unpacking dos2unix (7.4.2-2) ...\n",
            "Setting up dos2unix (7.4.2-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "apt install dos2unix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pm7CThaaYud0",
      "metadata": {
        "id": "Pm7CThaaYud0"
      },
      "source": [
        "(not important, but useful to process the reference database)\n",
        "\n",
        "#### install swarm\n",
        "\n",
        "We could install `swarm` and `vsearch` using `conda`, but for\n",
        "educational purposes, let’s compile them ourselves. We will put their\n",
        "source code in the `/tmp` folder (this folder is cleaned automatically\n",
        "between reboots):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ScyNyt27Yud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScyNyt27Yud0",
        "outputId": "436b2cf9-990b-46fc-8f34-70d56faa7d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'swarm'...\n",
            "remote: Enumerating objects: 13671, done.\u001b[K\n",
            "remote: Counting objects: 100% (928/928), done.\u001b[K\n",
            "remote: Compressing objects: 100% (449/449), done.\u001b[K\n",
            "remote: Total 13671 (delta 664), reused 722 (delta 479), pack-reused 12743 (from 1)\u001b[K\n",
            "Receiving objects: 100% (13671/13671), 4.78 MiB | 14.69 MiB/s, done.\n",
            "Resolving deltas: 100% (10268/10268), done.\n",
            "make -C src swarm\n",
            "make[1]: Entering directory '/tmp/swarm/src'\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o algo.o algo.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o algod1.o algod1.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o arch.o arch.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o bloomflex.o bloomflex.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o bloompat.o bloompat.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o db.o db.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o derep.o derep.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o hashtable.o hashtable.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o nw.o nw.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o qgram.o qgram.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o scan.o scan.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o search16.o search16.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o search8.o search8.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o swarm.o swarm.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o util.o util.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o variants.o variants.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o zobrist.o zobrist.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/cigar.o utils/cigar.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/fatal.o utils/fatal.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/hashtable_size.o utils/hashtable_size.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/input_output.o utils/input_output.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/intrinsics_to_functions_aarch64.o utils/intrinsics_to_functions_aarch64.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/intrinsics_to_functions_ppc.o utils/intrinsics_to_functions_ppc.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/intrinsics_to_functions_x86_64.o utils/intrinsics_to_functions_x86_64.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/nt_codec.o utils/nt_codec.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/open_and_close_files.o utils/open_and_close_files.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/progress.o utils/progress.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic   -c -o utils/x86_cpu_features.o utils/x86_cpu_features.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic -mssse3 -c -o ssse3.o ssse3.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic -msse4.1 -c -o sse41.o sse41.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11 -Wall -Wextra -Wpedantic -mpopcnt -c -o popcnt.o popcnt.cc\n",
            "g++ -g -O3 -DNDEBUG -flto -march=x86-64 -mtune=generic -std=c++11  -o swarm algo.o algod1.o arch.o bloomflex.o bloompat.o db.o derep.o hashtable.o nw.o qgram.o scan.o search16.o search8.o swarm.o util.o variants.o zobrist.o  utils/cigar.o  utils/fatal.o  utils/hashtable_size.o  utils/input_output.o  utils/intrinsics_to_functions_aarch64.o  utils/intrinsics_to_functions_ppc.o  utils/intrinsics_to_functions_x86_64.o  utils/nt_codec.o  utils/open_and_close_files.o  utils/progress.o  utils/x86_cpu_features.o ssse3.o sse41.o popcnt.o -lpthread\n",
            "lto-wrapper: warning: using serial compilation of 2 LTRANS jobs\n",
            "mkdir -p ../bin\n",
            "cp -a swarm ../bin\n",
            "make[1]: Leaving directory '/tmp/swarm/src'\n",
            "/usr/bin/install -c bin/swarm /usr/local/bin\n",
            "/usr/bin/install -d /usr/local/share/man/man1\n",
            "/usr/bin/install -c man/swarm.1 /usr/local/share/man/man1\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd /tmp/\n",
        "git clone https://github.com/torognes/swarm.git\n",
        "cd ./swarm/\n",
        "make --jobs\n",
        "make install"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OYR6Uh8GYud0",
      "metadata": {
        "id": "OYR6Uh8GYud0"
      },
      "source": [
        "check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M2N5wfsuYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2N5wfsuYud0",
        "outputId": "8fe95314-cb1f-4504-9b66-266dbf3b3699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swarm 3.1.5\n",
            "Copyright (C) 2012-2024 Torbjorn Rognes and Frederic Mahe\n",
            "https://github.com/torognes/swarm\n",
            "\n",
            "Mahe F, Rognes T, Quince C, de Vargas C, Dunthorn M (2014)\n",
            "Swarm: robust and fast clustering method for amplicon-based studies\n",
            "PeerJ 2:e593 https://doi.org/10.7717/peerj.593\n",
            "\n",
            "Mahe F, Rognes T, Quince C, de Vargas C, Dunthorn M (2015)\n",
            "Swarm v2: highly-scalable and high-resolution amplicon clustering\n",
            "PeerJ 3:e1420 https://doi.org/10.7717/peerj.1420\n",
            "\n",
            "Mahe F, Czech L, Stamatakis A, Quince C, de Vargas C, Dunthorn M, Rognes T (2022)\n",
            "Swarm v3: towards tera-scale amplicon clustering\n",
            "Bioinformatics 38:1, 267-269 https://doi.org/10.1093/bioinformatics/btab493\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "swarm --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8U6EI-s-Yud0",
      "metadata": {
        "id": "8U6EI-s-Yud0"
      },
      "source": [
        "clean-up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_PMKXfhSYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PMKXfhSYud0",
        "outputId": "908af99a-b325-439e-e8fc-b9ad3d46d1ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "rm --recursive /tmp/swarm/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WDmIcBozYud0",
      "metadata": {
        "id": "WDmIcBozYud0"
      },
      "source": [
        "#### install vsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jii7qlyuYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jii7qlyuYud0",
        "outputId": "629aec39-7552-411e-b02f-d14e5aeee576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'vsearch'...\n",
            "remote: Enumerating objects: 22348, done.\u001b[K\n",
            "remote: Counting objects: 100% (6737/6737), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1761/1761), done.\u001b[K\n",
            "remote: Total 22348 (delta 5431), reused 6236 (delta 4975), pack-reused 15611 (from 2)\u001b[K\n",
            "Receiving objects: 100% (22348/22348), 7.00 MiB | 15.24 MiB/s, done.\n",
            "Resolving deltas: 100% (16775/16775), done.\n",
            "configure.ac:103: installing './compile'\n",
            "configure.ac:6: installing './config.guess'\n",
            "configure.ac:6: installing './config.sub'\n",
            "configure.ac:7: installing './install-sh'\n",
            "configure.ac:7: installing './missing'\n",
            "src/Makefile.am: installing './depcomp'\n",
            "checking build system type... x86_64-pc-linux-gnu\n",
            "checking host system type... x86_64-pc-linux-gnu\n",
            "checking target system type... x86_64-pc-linux-gnu\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a race-free mkdir -p... /usr/bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports nested variables... yes\n",
            "checking for g++... g++\n",
            "checking whether the C++ compiler works... yes\n",
            "checking for C++ compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether the compiler supports GNU C++... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking for g++ option to enable C++11 features... none needed\n",
            "checking whether make supports the include directive... yes (GNU style)\n",
            "checking dependency style of g++... gcc3\n",
            "checking for ranlib... ranlib\n",
            "checking for pthread_create in -lpthread... yes\n",
            "checking for dlopen in -ldl... yes\n",
            "checking for GetProcessMemoryInfo in -lpsapi... no\n",
            "checking for stdio.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for getopt.h... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for float.h... yes\n",
            "checking for regex.h... yes\n",
            "checking for ctype.h... yes\n",
            "checking for locale.h... yes\n",
            "checking for limits.h... yes\n",
            "checking for string.h... (cached) yes\n",
            "checking for sys/time.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for pthread.h... yes\n",
            "checking for inline... inline\n",
            "checking for size_t... yes\n",
            "checking for uint32_t... yes\n",
            "checking for int64_t... yes\n",
            "checking for uint64_t... yes\n",
            "checking for uint8_t... yes\n",
            "checking for memmove... yes\n",
            "checking for memcpy... yes\n",
            "checking for posix_memalign... yes\n",
            "checking for gettimeofday... yes\n",
            "checking for localtime... yes\n",
            "checking for memchr... yes\n",
            "checking for memset... yes\n",
            "checking for pow... yes\n",
            "checking for regcomp... yes\n",
            "checking for strcasecmp... yes\n",
            "checking for strchr... yes\n",
            "checking for strcspn... yes\n",
            "checking for sysinfo... yes\n",
            "checking for bzlib.h... yes\n",
            "checking for zlib.h... yes\n",
            "checking for ps2pdf... no\n",
            "configure: WARNING: *** ps2pdf is required to build a PDF version of the manual\n",
            "checking for windows.h... no\n",
            "checking for gcc... gcc\n",
            "checking whether the compiler supports GNU C... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to enable C11 features... none needed\n",
            "checking whether gcc understands -c and -o together... yes\n",
            "checking dependency style of gcc... gcc3\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating src/Makefile\n",
            "config.status: creating man/Makefile\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "make  all-recursive\n",
            "make[1]: Entering directory '/tmp/vsearch'\n",
            "Making all in src\n",
            "make[2]: Entering directory '/tmp/vsearch/src'\n",
            "depbase=`echo align_simd.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT align_simd.o -MD -MP -MF $depbase.Tpo -c -o align_simd.o align_simd.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo allpairs.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT allpairs.o -MD -MP -MF $depbase.Tpo -c -o allpairs.o allpairs.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo arch.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT arch.o -MD -MP -MF $depbase.Tpo -c -o arch.o arch.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo attributes.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT attributes.o -MD -MP -MF $depbase.Tpo -c -o attributes.o attributes.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo bitmap.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT bitmap.o -MD -MP -MF $depbase.Tpo -c -o bitmap.o bitmap.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo chimera.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT chimera.o -MD -MP -MF $depbase.Tpo -c -o chimera.o chimera.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo cluster.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT cluster.o -MD -MP -MF $depbase.Tpo -c -o cluster.o cluster.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo cut.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT cut.o -MD -MP -MF $depbase.Tpo -c -o cut.o cut.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo db.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT db.o -MD -MP -MF $depbase.Tpo -c -o db.o db.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo dbhash.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT dbhash.o -MD -MP -MF $depbase.Tpo -c -o dbhash.o dbhash.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo dbindex.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT dbindex.o -MD -MP -MF $depbase.Tpo -c -o dbindex.o dbindex.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo derep.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT derep.o -MD -MP -MF $depbase.Tpo -c -o derep.o derep.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo derep_prefix.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT derep_prefix.o -MD -MP -MF $depbase.Tpo -c -o derep_prefix.o derep_prefix.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo derep_smallmem.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT derep_smallmem.o -MD -MP -MF $depbase.Tpo -c -o derep_smallmem.o derep_smallmem.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo dynlibs.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT dynlibs.o -MD -MP -MF $depbase.Tpo -c -o dynlibs.o dynlibs.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo eestats.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT eestats.o -MD -MP -MF $depbase.Tpo -c -o eestats.o eestats.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo fasta2fastq.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT fasta2fastq.o -MD -MP -MF $depbase.Tpo -c -o fasta2fastq.o fasta2fastq.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo fasta.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT fasta.o -MD -MP -MF $depbase.Tpo -c -o fasta.o fasta.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo fastq.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT fastq.o -MD -MP -MF $depbase.Tpo -c -o fastq.o fastq.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo fastq_chars.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT fastq_chars.o -MD -MP -MF $depbase.Tpo -c -o fastq_chars.o fastq_chars.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo fastq_join.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT fastq_join.o -MD -MP -MF $depbase.Tpo -c -o fastq_join.o fastq_join.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo fastqops.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT fastqops.o -MD -MP -MF $depbase.Tpo -c -o fastqops.o fastqops.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo fastx.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT fastx.o -MD -MP -MF $depbase.Tpo -c -o fastx.o fastx.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo filter.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT filter.o -MD -MP -MF $depbase.Tpo -c -o filter.o filter.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo getseq.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT getseq.o -MD -MP -MF $depbase.Tpo -c -o getseq.o getseq.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo kmerhash.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT kmerhash.o -MD -MP -MF $depbase.Tpo -c -o kmerhash.o kmerhash.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo linmemalign.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT linmemalign.o -MD -MP -MF $depbase.Tpo -c -o linmemalign.o linmemalign.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo maps.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT maps.o -MD -MP -MF $depbase.Tpo -c -o maps.o maps.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo mask.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT mask.o -MD -MP -MF $depbase.Tpo -c -o mask.o mask.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo md5.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "gcc -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -O3 -MT md5.o -MD -MP -MF $depbase.Tpo -c -o md5.o md5.c &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo mergepairs.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT mergepairs.o -MD -MP -MF $depbase.Tpo -c -o mergepairs.o mergepairs.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo minheap.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT minheap.o -MD -MP -MF $depbase.Tpo -c -o minheap.o minheap.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo msa.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT msa.o -MD -MP -MF $depbase.Tpo -c -o msa.o msa.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo orient.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT orient.o -MD -MP -MF $depbase.Tpo -c -o orient.o orient.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo otutable.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT otutable.o -MD -MP -MF $depbase.Tpo -c -o otutable.o otutable.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo rereplicate.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT rereplicate.o -MD -MP -MF $depbase.Tpo -c -o rereplicate.o rereplicate.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo results.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT results.o -MD -MP -MF $depbase.Tpo -c -o results.o results.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo search.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT search.o -MD -MP -MF $depbase.Tpo -c -o search.o search.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo searchcore.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT searchcore.o -MD -MP -MF $depbase.Tpo -c -o searchcore.o searchcore.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo search_exact.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT search_exact.o -MD -MP -MF $depbase.Tpo -c -o search_exact.o search_exact.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo sff_convert.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT sff_convert.o -MD -MP -MF $depbase.Tpo -c -o sff_convert.o sff_convert.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo sha1.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "gcc -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -O3 -MT sha1.o -MD -MP -MF $depbase.Tpo -c -o sha1.o sha1.c &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo showalign.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT showalign.o -MD -MP -MF $depbase.Tpo -c -o showalign.o showalign.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo shuffle.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT shuffle.o -MD -MP -MF $depbase.Tpo -c -o shuffle.o shuffle.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo sintax.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT sintax.o -MD -MP -MF $depbase.Tpo -c -o sintax.o sintax.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo sortbylength.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT sortbylength.o -MD -MP -MF $depbase.Tpo -c -o sortbylength.o sortbylength.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo sortbysize.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT sortbysize.o -MD -MP -MF $depbase.Tpo -c -o sortbysize.o sortbysize.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo subsample.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT subsample.o -MD -MP -MF $depbase.Tpo -c -o subsample.o subsample.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo tax.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT tax.o -MD -MP -MF $depbase.Tpo -c -o tax.o tax.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo udb.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT udb.o -MD -MP -MF $depbase.Tpo -c -o udb.o udb.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo unique.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT unique.o -MD -MP -MF $depbase.Tpo -c -o unique.o unique.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo userfields.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT userfields.o -MD -MP -MF $depbase.Tpo -c -o userfields.o userfields.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo util.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT util.o -MD -MP -MF $depbase.Tpo -c -o util.o util.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo vsearch.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT vsearch.o -MD -MP -MF $depbase.Tpo -c -o vsearch.o vsearch.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -Wno-sign-compare -O3 -MT libcityhash_a-city.o -MD -MP -MF .deps/libcityhash_a-city.Tpo -c -o libcityhash_a-city.o `test -f 'city.cc' || echo './'`city.cc\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -mssse3 -DSSSE3 -O3 -MT libcpu_ssse3_a-cpu.o -MD -MP -MF .deps/libcpu_ssse3_a-cpu.Tpo -c -o libcpu_ssse3_a-cpu.o `test -f 'cpu.cc' || echo './'`cpu.cc\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -msse2 -O3 -MT libcpu_sse2_a-cpu.o -MD -MP -MF .deps/libcpu_sse2_a-cpu.Tpo -c -o libcpu_sse2_a-cpu.o `test -f 'cpu.cc' || echo './'`cpu.cc\n",
            "depbase=`echo utils/maps.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT utils/maps.o -MD -MP -MF $depbase.Tpo -c -o utils/maps.o utils/maps.cpp &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "depbase=`echo utils/seqcmp.o | sed 's|[^/]*$|.deps/&|;s|\\.o$||'`;\\\n",
            "g++ -DHAVE_CONFIG_H -I. -I..    -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3 -MT utils/seqcmp.o -MD -MP -MF $depbase.Tpo -c -o utils/seqcmp.o utils/seqcmp.cc &&\\\n",
            "mv -f $depbase.Tpo $depbase.Po\n",
            "mv -f .deps/libcityhash_a-city.Tpo .deps/libcityhash_a-city.Po\n",
            "rm -f libcityhash.a\n",
            "ar cru libcityhash.a libcityhash_a-city.o \n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "ranlib libcityhash.a\n",
            "mv -f .deps/libcpu_ssse3_a-cpu.Tpo .deps/libcpu_ssse3_a-cpu.Po\n",
            "rm -f libcpu_ssse3.a\n",
            "ar cru libcpu_ssse3.a libcpu_ssse3_a-cpu.o  \n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "mv -f .deps/libcpu_sse2_a-cpu.Tpo .deps/libcpu_sse2_a-cpu.Po\n",
            "rm -f libcpu_sse2.a\n",
            "ranlib libcpu_ssse3.a\n",
            "ar cru libcpu_sse2.a libcpu_sse2_a-cpu.o  \n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "ranlib libcpu_sse2.a\n",
            "g++ -Wall -Wextra -Wpedantic    -march=x86-64 -mtune=generic  -DNDEBUG -std=c++11 -O3   -o ../bin/vsearch  align_simd.o allpairs.o arch.o attributes.o bitmap.o chimera.o cluster.o cut.o db.o dbhash.o dbindex.o derep.o derep_prefix.o derep_smallmem.o dynlibs.o eestats.o fasta2fastq.o fasta.o fastq.o fastq_chars.o fastq_join.o fastqops.o fastx.o filter.o getseq.o kmerhash.o linmemalign.o maps.o mask.o md5.o mergepairs.o minheap.o msa.o orient.o otutable.o rereplicate.o results.o search.o searchcore.o search_exact.o sff_convert.o sha1.o showalign.o shuffle.o sintax.o sortbylength.o sortbysize.o subsample.o tax.o udb.o unique.o userfields.o util.o utils/maps.o utils/seqcmp.o vsearch.o libcityhash.a libcpu_ssse3.a libcpu_sse2.a -ldl -lpthread \n",
            "make[2]: Leaving directory '/tmp/vsearch/src'\n",
            "Making all in man\n",
            "make[2]: Entering directory '/tmp/vsearch/man'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/tmp/vsearch/man'\n",
            "make[2]: Entering directory '/tmp/vsearch'\n",
            "make[2]: Leaving directory '/tmp/vsearch'\n",
            "make[1]: Leaving directory '/tmp/vsearch'\n",
            "Making install in src\n",
            "make[1]: Entering directory '/tmp/vsearch/src'\n",
            "make[2]: Entering directory '/tmp/vsearch/src'\n",
            " /usr/bin/mkdir -p '/usr/local/bin'\n",
            "  /usr/bin/install -c ../bin/vsearch '/usr/local/bin'\n",
            "make[2]: Nothing to be done for 'install-data-am'.\n",
            "make[2]: Leaving directory '/tmp/vsearch/src'\n",
            "make[1]: Leaving directory '/tmp/vsearch/src'\n",
            "Making install in man\n",
            "make[1]: Entering directory '/tmp/vsearch/man'\n",
            "make[2]: Entering directory '/tmp/vsearch/man'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            " /usr/bin/mkdir -p '/usr/local/share/doc/vsearch'\n",
            " /usr/bin/mkdir -p '/usr/local/share/man/man1'\n",
            " /usr/bin/install -c -m 644 vsearch.1 '/usr/local/share/man/man1'\n",
            "make[2]: Leaving directory '/tmp/vsearch/man'\n",
            "make[1]: Leaving directory '/tmp/vsearch/man'\n",
            "make[1]: Entering directory '/tmp/vsearch'\n",
            "make[2]: Entering directory '/tmp/vsearch'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            "make[2]: Nothing to be done for 'install-data-am'.\n",
            "make[2]: Leaving directory '/tmp/vsearch'\n",
            "make[1]: Leaving directory '/tmp/vsearch'\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd /tmp/\n",
        "git clone https://github.com/torognes/vsearch.git\n",
        "cd ./vsearch/\n",
        "./autogen.sh\n",
        "./configure CFLAGS=\"-O3\" CXXFLAGS=\"-O3\"\n",
        "make --jobs\n",
        "make install"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "otgmY3prYud0",
      "metadata": {
        "id": "otgmY3prYud0"
      },
      "source": [
        "check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FfeYgGsAYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfeYgGsAYud0",
        "outputId": "3a140493-1d94-4405-ce21-a8c2f3ad1490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vsearch v2.30.0_linux_x86_64, 12.7GB RAM, 2 cores\n",
            "https://github.com/torognes/vsearch\n",
            "\n",
            "Rognes T, Flouri T, Nichols B, Quince C, Mahe F (2016)\n",
            "VSEARCH: a versatile open source tool for metagenomics\n",
            "PeerJ 4:e2584 doi: 10.7717/peerj.2584 https://doi.org/10.7717/peerj.2584\n",
            "\n",
            "Compiled with support for gzip-compressed files, and the library is loaded.\n",
            "zlib version 1.2.11, compile flags a9\n",
            "Compiled with support for bzip2-compressed files, and the library is loaded.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "vsearch --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wma_cc4yYud0",
      "metadata": {
        "id": "wma_cc4yYud0"
      },
      "source": [
        "clean-up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZAsj107cYud0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAsj107cYud0",
        "outputId": "87541fd6-94d5-4f79-dc8b-00d628fcc8df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "rm --recursive /tmp/vsearch/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r6JiQna7Yud0",
      "metadata": {
        "id": "r6JiQna7Yud0"
      },
      "source": [
        "#### install python scripts\n",
        "\n",
        "In the second part of the pipeline, we are going to use four python\n",
        "scripts to build and update occurrence tables, and to compute\n",
        "last-common ancestor taxonomic assignments.\n",
        "\n",
        "The scripts are available on GitHub, let’s download the latest versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I2EXMEDUYud1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2EXMEDUYud1",
        "outputId": "06c5e057-639b-425d-9510-31e3386f4f8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-05 21:14:53--  https://raw.githubusercontent.com/frederic-mahe/fred-metabarcoding-pipeline/master/src/OTU_cleaver.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15266 (15K) [text/plain]\n",
            "Saving to: ‘OTU_cleaver.py’\n",
            "\n",
            "OTU_cleaver.py      100%[===================>]  14.91K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-05 21:14:53 (25.5 MB/s) - ‘OTU_cleaver.py’ saved [15266/15266]\n",
            "\n",
            "--2025-04-05 21:14:53--  https://raw.githubusercontent.com/frederic-mahe/fred-metabarcoding-pipeline/master/src/OTU_contingency_table_filtered.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13368 (13K) [text/plain]\n",
            "Saving to: ‘OTU_contingency_table_filtered.py’\n",
            "\n",
            "OTU_contingency_tab 100%[===================>]  13.05K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-05 21:14:53 (20.5 MB/s) - ‘OTU_contingency_table_filtered.py’ saved [13368/13368]\n",
            "\n",
            "--2025-04-05 21:14:53--  https://raw.githubusercontent.com/frederic-mahe/fred-metabarcoding-pipeline/master/src/OTU_table_updater.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4197 (4.1K) [text/plain]\n",
            "Saving to: ‘OTU_table_updater.py’\n",
            "\n",
            "OTU_table_updater.p 100%[===================>]   4.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-05 21:14:54 (41.1 MB/s) - ‘OTU_table_updater.py’ saved [4197/4197]\n",
            "\n",
            "--2025-04-05 21:14:54--  https://raw.githubusercontent.com/frederic-mahe/stampa/master/stampa_merge.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4467 (4.4K) [text/plain]\n",
            "Saving to: ‘stampa_merge.py’\n",
            "\n",
            "stampa_merge.py     100%[===================>]   4.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-05 21:14:54 (49.2 MB/s) - ‘stampa_merge.py’ saved [4467/4467]\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./src/\n",
        "\n",
        "# occurrence table creation\n",
        "URL=\"https://raw.githubusercontent.com/frederic-mahe/fred-metabarcoding-pipeline/master/src\"\n",
        "for SCRIPT in \"OTU_cleaver\" \"OTU_contingency_table_filtered\" \"OTU_table_updater\" ; do\n",
        "    wget --continue \"${URL}/${SCRIPT}.py\"\n",
        "done\n",
        "\n",
        "# taxonomic assignment\n",
        "URL=\"https://raw.githubusercontent.com/frederic-mahe/stampa/master\"\n",
        "wget --continue \"${URL}/stampa_merge.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y58GWC8VYud1",
      "metadata": {
        "id": "Y58GWC8VYud1"
      },
      "source": [
        "### sequencing data\n",
        "\n",
        "Today, we are going to use a subset of the Neotropical Forest Soil\n",
        "dataset\n",
        "([PRJNA317860](https://www.ebi.ac.uk/ena/browser/view/PRJNA317860);\n",
        "[Mahé et al., 2017](https://doi.org/10.1038/s41559-017-0091)),\n",
        "corresponding to the following ENA/SRA run accessions:\n",
        "\n",
        "| runs        | sample |\n",
        "|-------------|--------|\n",
        "| SRR23272700 | B070   |\n",
        "| SRR23272716 | B030   |\n",
        "| SRR23272737 | B100   |\n",
        "| SRR23272741 | B060   |\n",
        "| SRR23272752 | B050   |\n",
        "| SRR23272767 | L040   |\n",
        "| SRR23272778 | L030   |\n",
        "| SRR23272788 | B090   |\n",
        "| SRR23272799 | B080   |\n",
        "| SRR23272803 | B040   |\n",
        "| SRR23272822 | L080   |\n",
        "| SRR23272833 | L070   |\n",
        "| SRR23272848 | L020   |\n",
        "| SRR23272859 | L010   |\n",
        "| SRR23272860 | B020   |\n",
        "| SRR23272861 | B010   |\n",
        "| SRR23272874 | L090   |\n",
        "| SRR23272881 | L100   |\n",
        "| SRR23272890 | L060   |\n",
        "| SRR23272901 | L050   |\n",
        "\n",
        "and subsampled at 1%, using `vsearch`:\n",
        "\n",
        "``` shell\n",
        "function subsample() {\n",
        "    local -ri SEED=1\n",
        "    local -r PERCENTAGE=\"1.0\"\n",
        "    local -r SUBSAMPLED_FASTQ=\"$(sed 's/NG-7070_// ; s/_lib.*_1976//' <<< ${1})\"\n",
        "\n",
        "    vsearch \\\n",
        "        --fastx_subsample \"${1}\" \\\n",
        "        --randseed \"${SEED}\" \\\n",
        "        --sample_pct \"${PERCENTAGE}\" \\\n",
        "        --quiet \\\n",
        "        --fastqout - | \\\n",
        "        gzip - > \"${SUBSAMPLED_FASTQ}\"\n",
        "}\n",
        "\n",
        "\n",
        "export -f subsample\n",
        "\n",
        "find . -name \"NG-7070_*.fastq.gz\" -type f -exec bash -c 'subsample \"$0\"' {} \\;\n",
        "```\n",
        "\n",
        "Subsampling is useful when you are developing and testing a new\n",
        "pipeline. The dataset is smaller but remains realistic, allowing for a\n",
        "faster development cycle. With `vsearch`, you can also subsample down to\n",
        "a particular number of reads.\n",
        "\n",
        "Combining `vsearch` with the command `find` offers a very powerful and\n",
        "robust way to find and subsample all fastq files (it will find every\n",
        "file, including files in sub-folders). You might find a loop-based\n",
        "approach easier to read:\n",
        "\n",
        "``` shell\n",
        "for FASTQ in \"NG-7070_*.fastq.gz\" ; do\n",
        "    subsample \"${FASTQ}\"\n",
        "done\n",
        "```\n",
        "\n",
        "Note: in a pair of R1 and R2 fastq files, both files have the same\n",
        "number of reads, so using a fix seed (i.e., not zero) guarantees that\n",
        "subsamplings results for both R1 and R2 fastq files are identical (same\n",
        "number of reads, same reads, in the same order)\n",
        "\n",
        "These 20 runs represent 4 GB of compressed data. To save time and\n",
        "energy, we are going to download the subsampled files directly (roughly\n",
        "40 MB in total). The files are hosted on my GitHub account:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K4O6p6yGYud1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4O6p6yGYud1",
        "outputId": "5c66a604-c06c-4762-fc72-441cc3d2b20e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-05 21:15:11--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/MD5SUM\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/MD5SUM [following]\n",
            "--2025-04-05 21:15:11--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/MD5SUM\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2080 (2.0K) [text/plain]\n",
            "Saving to: ‘MD5SUM’\n",
            "\n",
            "MD5SUM              100%[===================>]   2.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-05 21:15:11 (31.7 MB/s) - ‘MD5SUM’ saved [2080/2080]\n",
            "\n",
            "--2025-04-05 21:15:11--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B010_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B010_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:11--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B010_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 480601 (469K) [application/octet-stream]\n",
            "Saving to: ‘B010_1_1.fastq.gz’\n",
            "\n",
            "B010_1_1.fastq.gz   100%[===================>] 469.34K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-05 21:15:12 (10.4 MB/s) - ‘B010_1_1.fastq.gz’ saved [480601/480601]\n",
            "\n",
            "--2025-04-05 21:15:12--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B010_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B010_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:12--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B010_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 602837 (589K) [application/octet-stream]\n",
            "Saving to: ‘B010_1_2.fastq.gz’\n",
            "\n",
            "B010_1_2.fastq.gz   100%[===================>] 588.71K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:12 (11.0 MB/s) - ‘B010_1_2.fastq.gz’ saved [602837/602837]\n",
            "\n",
            "--2025-04-05 21:15:12--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B020_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B020_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:12--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B020_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 975520 (953K) [application/octet-stream]\n",
            "Saving to: ‘B020_1_1.fastq.gz’\n",
            "\n",
            "B020_1_1.fastq.gz   100%[===================>] 952.66K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:13 (16.4 MB/s) - ‘B020_1_1.fastq.gz’ saved [975520/975520]\n",
            "\n",
            "--2025-04-05 21:15:13--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B020_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B020_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:13--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B020_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1134826 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘B020_1_2.fastq.gz’\n",
            "\n",
            "B020_1_2.fastq.gz   100%[===================>]   1.08M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:13 (18.3 MB/s) - ‘B020_1_2.fastq.gz’ saved [1134826/1134826]\n",
            "\n",
            "--2025-04-05 21:15:13--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B030_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B030_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:14--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B030_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 714226 (697K) [application/octet-stream]\n",
            "Saving to: ‘B030_1_1.fastq.gz’\n",
            "\n",
            "B030_1_1.fastq.gz   100%[===================>] 697.49K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:14 (14.3 MB/s) - ‘B030_1_1.fastq.gz’ saved [714226/714226]\n",
            "\n",
            "--2025-04-05 21:15:14--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B030_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B030_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:14--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B030_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 883838 (863K) [application/octet-stream]\n",
            "Saving to: ‘B030_1_2.fastq.gz’\n",
            "\n",
            "B030_1_2.fastq.gz   100%[===================>] 863.12K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:14 (15.0 MB/s) - ‘B030_1_2.fastq.gz’ saved [883838/883838]\n",
            "\n",
            "--2025-04-05 21:15:14--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B040_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B040_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:14--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B040_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1066584 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘B040_1_1.fastq.gz’\n",
            "\n",
            "B040_1_1.fastq.gz   100%[===================>]   1.02M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:15 (17.2 MB/s) - ‘B040_1_1.fastq.gz’ saved [1066584/1066584]\n",
            "\n",
            "--2025-04-05 21:15:15--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B040_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B040_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:15--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B040_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1307365 (1.2M) [application/octet-stream]\n",
            "Saving to: ‘B040_1_2.fastq.gz’\n",
            "\n",
            "B040_1_2.fastq.gz   100%[===================>]   1.25M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:15 (19.2 MB/s) - ‘B040_1_2.fastq.gz’ saved [1307365/1307365]\n",
            "\n",
            "--2025-04-05 21:15:15--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B050_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B050_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:15--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B050_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 458325 (448K) [application/octet-stream]\n",
            "Saving to: ‘B050_1_1.fastq.gz’\n",
            "\n",
            "B050_1_1.fastq.gz   100%[===================>] 447.58K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-05 21:15:16 (9.83 MB/s) - ‘B050_1_1.fastq.gz’ saved [458325/458325]\n",
            "\n",
            "--2025-04-05 21:15:16--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B050_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B050_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:16--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B050_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 532698 (520K) [application/octet-stream]\n",
            "Saving to: ‘B050_1_2.fastq.gz’\n",
            "\n",
            "B050_1_2.fastq.gz   100%[===================>] 520.21K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:16 (10.9 MB/s) - ‘B050_1_2.fastq.gz’ saved [532698/532698]\n",
            "\n",
            "--2025-04-05 21:15:16--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B060_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B060_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:16--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B060_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1216985 (1.2M) [application/octet-stream]\n",
            "Saving to: ‘B060_1_1.fastq.gz’\n",
            "\n",
            "B060_1_1.fastq.gz   100%[===================>]   1.16M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:17 (18.0 MB/s) - ‘B060_1_1.fastq.gz’ saved [1216985/1216985]\n",
            "\n",
            "--2025-04-05 21:15:17--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B060_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B060_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:17--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B060_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1468873 (1.4M) [application/octet-stream]\n",
            "Saving to: ‘B060_1_2.fastq.gz’\n",
            "\n",
            "B060_1_2.fastq.gz   100%[===================>]   1.40M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:17 (21.4 MB/s) - ‘B060_1_2.fastq.gz’ saved [1468873/1468873]\n",
            "\n",
            "--2025-04-05 21:15:17--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B070_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B070_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:17--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B070_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177387 (173K) [application/octet-stream]\n",
            "Saving to: ‘B070_1_1.fastq.gz’\n",
            "\n",
            "B070_1_1.fastq.gz   100%[===================>] 173.23K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-05 21:15:18 (4.94 MB/s) - ‘B070_1_1.fastq.gz’ saved [177387/177387]\n",
            "\n",
            "--2025-04-05 21:15:18--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B070_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B070_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:18--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B070_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 205803 (201K) [application/octet-stream]\n",
            "Saving to: ‘B070_1_2.fastq.gz’\n",
            "\n",
            "B070_1_2.fastq.gz   100%[===================>] 200.98K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-05 21:15:18 (5.58 MB/s) - ‘B070_1_2.fastq.gz’ saved [205803/205803]\n",
            "\n",
            "--2025-04-05 21:15:18--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B080_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B080_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:18--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B080_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1086708 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘B080_1_1.fastq.gz’\n",
            "\n",
            "B080_1_1.fastq.gz   100%[===================>]   1.04M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:18 (17.8 MB/s) - ‘B080_1_1.fastq.gz’ saved [1086708/1086708]\n",
            "\n",
            "--2025-04-05 21:15:18--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B080_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B080_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:19--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B080_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1367922 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘B080_1_2.fastq.gz’\n",
            "\n",
            "B080_1_2.fastq.gz   100%[===================>]   1.30M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:19 (19.5 MB/s) - ‘B080_1_2.fastq.gz’ saved [1367922/1367922]\n",
            "\n",
            "--2025-04-05 21:15:19--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B090_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B090_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:19--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B090_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1320317 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘B090_1_1.fastq.gz’\n",
            "\n",
            "B090_1_1.fastq.gz   100%[===================>]   1.26M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:20 (18.9 MB/s) - ‘B090_1_1.fastq.gz’ saved [1320317/1320317]\n",
            "\n",
            "--2025-04-05 21:15:20--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B090_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B090_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:20--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B090_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1590836 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘B090_1_2.fastq.gz’\n",
            "\n",
            "B090_1_2.fastq.gz   100%[===================>]   1.52M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:20 (22.5 MB/s) - ‘B090_1_2.fastq.gz’ saved [1590836/1590836]\n",
            "\n",
            "--2025-04-05 21:15:20--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B100_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B100_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:21--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B100_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1559055 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘B100_1_1.fastq.gz’\n",
            "\n",
            "B100_1_1.fastq.gz   100%[===================>]   1.49M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:21 (22.0 MB/s) - ‘B100_1_1.fastq.gz’ saved [1559055/1559055]\n",
            "\n",
            "--2025-04-05 21:15:21--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/B100_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B100_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:21--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/B100_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1974958 (1.9M) [application/octet-stream]\n",
            "Saving to: ‘B100_1_2.fastq.gz’\n",
            "\n",
            "B100_1_2.fastq.gz   100%[===================>]   1.88M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:21 (27.5 MB/s) - ‘B100_1_2.fastq.gz’ saved [1974958/1974958]\n",
            "\n",
            "--2025-04-05 21:15:21--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L010_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L010_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:22--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L010_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957849 (935K) [application/octet-stream]\n",
            "Saving to: ‘L010_1_1.fastq.gz’\n",
            "\n",
            "L010_1_1.fastq.gz   100%[===================>] 935.40K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:22 (15.2 MB/s) - ‘L010_1_1.fastq.gz’ saved [957849/957849]\n",
            "\n",
            "--2025-04-05 21:15:22--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L010_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L010_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:22--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L010_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1169290 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘L010_1_2.fastq.gz’\n",
            "\n",
            "L010_1_2.fastq.gz   100%[===================>]   1.11M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:22 (19.0 MB/s) - ‘L010_1_2.fastq.gz’ saved [1169290/1169290]\n",
            "\n",
            "--2025-04-05 21:15:22--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L020_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L020_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:23--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L020_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 529643 (517K) [application/octet-stream]\n",
            "Saving to: ‘L020_1_1.fastq.gz’\n",
            "\n",
            "L020_1_1.fastq.gz   100%[===================>] 517.23K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:23 (10.6 MB/s) - ‘L020_1_1.fastq.gz’ saved [529643/529643]\n",
            "\n",
            "--2025-04-05 21:15:23--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L020_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L020_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:23--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L020_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 641052 (626K) [application/octet-stream]\n",
            "Saving to: ‘L020_1_2.fastq.gz’\n",
            "\n",
            "L020_1_2.fastq.gz   100%[===================>] 626.03K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:23 (11.3 MB/s) - ‘L020_1_2.fastq.gz’ saved [641052/641052]\n",
            "\n",
            "--2025-04-05 21:15:23--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L030_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L030_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:23--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L030_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 557121 (544K) [application/octet-stream]\n",
            "Saving to: ‘L030_1_1.fastq.gz’\n",
            "\n",
            "L030_1_1.fastq.gz   100%[===================>] 544.06K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:24 (11.4 MB/s) - ‘L030_1_1.fastq.gz’ saved [557121/557121]\n",
            "\n",
            "--2025-04-05 21:15:24--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L030_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L030_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:24--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L030_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 651736 (636K) [application/octet-stream]\n",
            "Saving to: ‘L030_1_2.fastq.gz’\n",
            "\n",
            "L030_1_2.fastq.gz   100%[===================>] 636.46K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:24 (11.6 MB/s) - ‘L030_1_2.fastq.gz’ saved [651736/651736]\n",
            "\n",
            "--2025-04-05 21:15:24--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L040_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L040_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:24--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L040_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1136628 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘L040_1_1.fastq.gz’\n",
            "\n",
            "L040_1_1.fastq.gz   100%[===================>]   1.08M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:25 (18.0 MB/s) - ‘L040_1_1.fastq.gz’ saved [1136628/1136628]\n",
            "\n",
            "--2025-04-05 21:15:25--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L040_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L040_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:25--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L040_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1381245 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘L040_1_2.fastq.gz’\n",
            "\n",
            "L040_1_2.fastq.gz   100%[===================>]   1.32M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-04-05 21:15:25 (14.2 MB/s) - ‘L040_1_2.fastq.gz’ saved [1381245/1381245]\n",
            "\n",
            "--2025-04-05 21:15:25--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L050_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L050_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:25--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L050_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1129503 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘L050_1_1.fastq.gz’\n",
            "\n",
            "L050_1_1.fastq.gz   100%[===================>]   1.08M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:26 (18.6 MB/s) - ‘L050_1_1.fastq.gz’ saved [1129503/1129503]\n",
            "\n",
            "--2025-04-05 21:15:26--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L050_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L050_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:26--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L050_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356660 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘L050_1_2.fastq.gz’\n",
            "\n",
            "L050_1_2.fastq.gz   100%[===================>]   1.29M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:26 (19.6 MB/s) - ‘L050_1_2.fastq.gz’ saved [1356660/1356660]\n",
            "\n",
            "--2025-04-05 21:15:26--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L060_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L060_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:27--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L060_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1097442 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘L060_1_1.fastq.gz’\n",
            "\n",
            "L060_1_1.fastq.gz   100%[===================>]   1.05M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:27 (17.6 MB/s) - ‘L060_1_1.fastq.gz’ saved [1097442/1097442]\n",
            "\n",
            "--2025-04-05 21:15:27--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L060_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L060_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:27--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L060_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1315029 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘L060_1_2.fastq.gz’\n",
            "\n",
            "L060_1_2.fastq.gz   100%[===================>]   1.25M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:27 (19.4 MB/s) - ‘L060_1_2.fastq.gz’ saved [1315029/1315029]\n",
            "\n",
            "--2025-04-05 21:15:27--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L070_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L070_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:28--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L070_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 975279 (952K) [application/octet-stream]\n",
            "Saving to: ‘L070_1_1.fastq.gz’\n",
            "\n",
            "L070_1_1.fastq.gz   100%[===================>] 952.42K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:28 (16.6 MB/s) - ‘L070_1_1.fastq.gz’ saved [975279/975279]\n",
            "\n",
            "--2025-04-05 21:15:28--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L070_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L070_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:28--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L070_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1182196 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘L070_1_2.fastq.gz’\n",
            "\n",
            "L070_1_2.fastq.gz   100%[===================>]   1.13M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:28 (17.7 MB/s) - ‘L070_1_2.fastq.gz’ saved [1182196/1182196]\n",
            "\n",
            "--2025-04-05 21:15:28--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L080_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L080_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:29--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L080_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 544960 (532K) [application/octet-stream]\n",
            "Saving to: ‘L080_1_1.fastq.gz’\n",
            "\n",
            "L080_1_1.fastq.gz   100%[===================>] 532.19K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:29 (10.9 MB/s) - ‘L080_1_1.fastq.gz’ saved [544960/544960]\n",
            "\n",
            "--2025-04-05 21:15:29--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L080_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L080_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:29--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L080_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657781 (642K) [application/octet-stream]\n",
            "Saving to: ‘L080_1_2.fastq.gz’\n",
            "\n",
            "L080_1_2.fastq.gz   100%[===================>] 642.36K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-05 21:15:29 (11.8 MB/s) - ‘L080_1_2.fastq.gz’ saved [657781/657781]\n",
            "\n",
            "--2025-04-05 21:15:29--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L090_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L090_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:30--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L090_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1080526 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘L090_1_1.fastq.gz’\n",
            "\n",
            "L090_1_1.fastq.gz   100%[===================>]   1.03M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:30 (16.4 MB/s) - ‘L090_1_1.fastq.gz’ saved [1080526/1080526]\n",
            "\n",
            "--2025-04-05 21:15:30--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L090_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L090_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:30--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L090_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1327505 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘L090_1_2.fastq.gz’\n",
            "\n",
            "L090_1_2.fastq.gz   100%[===================>]   1.27M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:30 (20.2 MB/s) - ‘L090_1_2.fastq.gz’ saved [1327505/1327505]\n",
            "\n",
            "--2025-04-05 21:15:30--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L100_1_1.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L100_1_1.fastq.gz [following]\n",
            "--2025-04-05 21:15:31--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L100_1_1.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1250031 (1.2M) [application/octet-stream]\n",
            "Saving to: ‘L100_1_1.fastq.gz’\n",
            "\n",
            "L100_1_1.fastq.gz   100%[===================>]   1.19M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-05 21:15:31 (19.1 MB/s) - ‘L100_1_1.fastq.gz’ saved [1250031/1250031]\n",
            "\n",
            "--2025-04-05 21:15:31--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data/L100_1_2.fastq.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L100_1_2.fastq.gz [following]\n",
            "--2025-04-05 21:15:31--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/data/L100_1_2.fastq.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1516770 (1.4M) [application/octet-stream]\n",
            "Saving to: ‘L100_1_2.fastq.gz’\n",
            "\n",
            "L100_1_2.fastq.gz   100%[===================>]   1.45M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-05 21:15:32 (22.3 MB/s) - ‘L100_1_2.fastq.gz’ saved [1516770/1516770]\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./data/\n",
        "\n",
        "URL=\"https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data\"\n",
        "\n",
        "wget \"${URL}/MD5SUM\"\n",
        "for SAMPLE in B010 B020 B030 B040 B050 B060 B070 B080 B090 B100 \\\n",
        "              L010 L020 L030 L040 L050 L060 L070 L080 L090 L100 ; do\n",
        "    for READ in 1 2 ; do\n",
        "        wget --continue \"${URL}/${SAMPLE}_1_${READ}.fastq.gz\"\n",
        "    done\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SVYfUm_KYud1",
      "metadata": {
        "id": "SVYfUm_KYud1"
      },
      "source": [
        "check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xKH1U0pYYud1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKH1U0pYYud1",
        "outputId": "e6469549-8a01-43d2-da67-d9f4feeeee0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B010_1_1.fastq.gz: OK\n",
            "B010_1_2.fastq.gz: OK\n",
            "B020_1_1.fastq.gz: OK\n",
            "B020_1_2.fastq.gz: OK\n",
            "B030_1_1.fastq.gz: OK\n",
            "B030_1_2.fastq.gz: OK\n",
            "B040_1_1.fastq.gz: OK\n",
            "B040_1_2.fastq.gz: OK\n",
            "B050_1_1.fastq.gz: OK\n",
            "B050_1_2.fastq.gz: OK\n",
            "B060_1_1.fastq.gz: OK\n",
            "B060_1_2.fastq.gz: OK\n",
            "B070_1_1.fastq.gz: OK\n",
            "B070_1_2.fastq.gz: OK\n",
            "B080_1_1.fastq.gz: OK\n",
            "B080_1_2.fastq.gz: OK\n",
            "B090_1_1.fastq.gz: OK\n",
            "B090_1_2.fastq.gz: OK\n",
            "B100_1_1.fastq.gz: OK\n",
            "B100_1_2.fastq.gz: OK\n",
            "L010_1_1.fastq.gz: OK\n",
            "L010_1_2.fastq.gz: OK\n",
            "L020_1_1.fastq.gz: OK\n",
            "L020_1_2.fastq.gz: OK\n",
            "L030_1_1.fastq.gz: OK\n",
            "L030_1_2.fastq.gz: OK\n",
            "L040_1_1.fastq.gz: OK\n",
            "L040_1_2.fastq.gz: OK\n",
            "L050_1_1.fastq.gz: OK\n",
            "L050_1_2.fastq.gz: OK\n",
            "L060_1_1.fastq.gz: OK\n",
            "L060_1_2.fastq.gz: OK\n",
            "L070_1_1.fastq.gz: OK\n",
            "L070_1_2.fastq.gz: OK\n",
            "L080_1_1.fastq.gz: OK\n",
            "L080_1_2.fastq.gz: OK\n",
            "L090_1_1.fastq.gz: OK\n",
            "L090_1_2.fastq.gz: OK\n",
            "L100_1_1.fastq.gz: OK\n",
            "L100_1_2.fastq.gz: OK\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./data/\n",
        "md5sum -c MD5SUM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ALtvz2kcYud4",
      "metadata": {
        "id": "ALtvz2kcYud4"
      },
      "source": [
        "Initial situation: fastq files are already demultiplexed, we have a pair\n",
        "of R1 and R2 files for each sample.\n",
        "\n",
        "Part 1: from fastq files to fasta files\n",
        "---------------------------------------\n",
        "\n",
        "![pipeline\n",
        "overview](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/diapo_pipeline_final_colour.png)\n",
        "\n",
        "The pipeline is divided into two parts. A first part where each sample\n",
        "is processed individually. And a second part where all samples are\n",
        "pooled to produce an occurrence table.\n",
        "\n",
        "In this first part of the pipeline, we will:\n",
        "\n",
        "1.  merge R1 and R2,\n",
        "2.  trim primers,\n",
        "3.  convert fastq to fasta,\n",
        "4.  extract expected errors (for a later *quality-based filtering*),\n",
        "5.  dereplicate fasta,\n",
        "6.  per-sample clustering (for a later *cluster cleaving*)\n",
        "\n",
        "The code below uses named pipes (`fifo`) to avoid writing intermediate\n",
        "results to mass storage. The goal is to speed up processing, and to make\n",
        "the code more modular and clearer. On the other hand, `fifo`s are tricky\n",
        "to use, as you must remember to launch producers and consumers in the\n",
        "background before running the last consumer.\n",
        "\n",
        "Don’t panic! While the script is running, we are going to look at each\n",
        "function and explain what it does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6uhqLrtWYud4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uhqLrtWYud4",
        "outputId": "0d489269-e8b9-438c-f222-c6912747f86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\t./B010_1_1.fastq.gz\n",
            "2\t./L100_1_1.fastq.gz\n",
            "3\t./B100_1_1.fastq.gz\n",
            "4\t./L020_1_1.fastq.gz\n",
            "5\t./L050_1_1.fastq.gz\n",
            "6\t./L010_1_1.fastq.gz\n",
            "7\t./B020_1_1.fastq.gz\n",
            "8\t./B050_1_1.fastq.gz\n",
            "9\t./L060_1_1.fastq.gz\n",
            "10\t./B030_1_1.fastq.gz\n",
            "11\t./L070_1_1.fastq.gz\n",
            "12\t./L040_1_1.fastq.gz\n",
            "13\t./L080_1_1.fastq.gz\n",
            "14\t./B090_1_1.fastq.gz\n",
            "15\t./B060_1_1.fastq.gz\n",
            "16\t./L030_1_1.fastq.gz\n",
            "17\t./B070_1_1.fastq.gz\n",
            "18\t./L090_1_1.fastq.gz\n",
            "19\t./B080_1_1.fastq.gz\n",
            "20\t./B040_1_1.fastq.gz\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./data/\n",
        "export LC_ALL=C\n",
        "\n",
        "## ------------------------------------------------------------ define variables\n",
        "declare -r PRIMER_F=\"CCAGCASCYGCGGTAATTCC\"\n",
        "declare -r PRIMER_R=\"ACTTTCGTTCTTGATYRA\"\n",
        "declare -r FASTQ_NAME_PATTERN=\"*_1.fastq.gz\"\n",
        "declare -ri THREADS=2\n",
        "declare -r CUTADAPT_OPTIONS=\"--minimum-length 32 --cores=${THREADS} --discard-untrimmed\"\n",
        "declare -r CUTADAPT=\"$(which cutadapt) ${CUTADAPT_OPTIONS}\"  # cutadapt 4.1 or more recent\n",
        "declare -r SWARM=\"$(which swarm)\"  # swarm 3.0 or more recent\n",
        "declare -r VSEARCH=\"$(which vsearch) --quiet\"  # vsearch 2.21.1 or more recent\n",
        "declare -ri ENCODING=33\n",
        "declare -r MIN_F=$(( ${#PRIMER_F} * 2 / 3 ))  # match is >= 2/3 of primer length\n",
        "declare -r MIN_R=$(( ${#PRIMER_R} * 2 / 3 ))\n",
        "declare -r FIFOS=$(echo fifo_{merged,trimmed}_fastq fifo_filtered_fasta{,_bis})\n",
        "declare -i TICKER=0\n",
        "\n",
        "## ------------------------------------------------------------------- functions\n",
        "revcomp() {\n",
        "    # reverse-complement a DNA/RNA IUPAC string\n",
        "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
        "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
        "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
        "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
        "}\n",
        "\n",
        "merge_fastq_pair() {\n",
        "    ${VSEARCH} \\\n",
        "        --threads \"${THREADS}\" \\\n",
        "        --fastq_mergepairs \"${FORWARD}\" \\\n",
        "        --reverse \"${REVERSE}\" \\\n",
        "        --fastq_ascii \"${ENCODING}\" \\\n",
        "        --fastq_allowmergestagger \\\n",
        "        --fastqout fifo_merged_fastq 2> \"${SAMPLE}.log\" &\n",
        "}\n",
        "\n",
        "trim_primers() {\n",
        "    # search forward primer in both normal and revcomp: now all reads\n",
        "    # are in the same orientation\n",
        "    ${CUTADAPT} \\\n",
        "        --revcomp \\\n",
        "        --front \"${PRIMER_F};rightmost\" \\\n",
        "        --overlap \"${MIN_F}\" fifo_merged_fastq 2>> \"${SAMPLE}.log\" | \\\n",
        "        ${CUTADAPT} \\\n",
        "            --adapter \"${ANTI_PRIMER_R}\" \\\n",
        "            --overlap \"${MIN_R}\" \\\n",
        "            --max-n 0 - > fifo_trimmed_fastq 2>> \"${SAMPLE}.log\" &\n",
        "}\n",
        "\n",
        "convert_fastq_to_fasta() {\n",
        "    # use SHA1 values as sequence names,\n",
        "    # compute expected error values (ee)\n",
        "    ${VSEARCH} \\\n",
        "        --fastq_filter fifo_trimmed_fastq \\\n",
        "        --relabel_sha1 \\\n",
        "        --fastq_ascii \"${ENCODING}\" \\\n",
        "        --eeout \\\n",
        "        --fasta_width 0 \\\n",
        "        --fastaout - 2>> \"${SAMPLE}.log\" | \\\n",
        "        tee fifo_filtered_fasta_bis > fifo_filtered_fasta &\n",
        "}\n",
        "\n",
        "extract_expected_error_values() {\n",
        "    # extract ee for future quality filtering (keep the lowest\n",
        "    # observed expected error value for each unique sequence)\n",
        "    local -ri length_of_sequence_IDs=40\n",
        "    paste - - < fifo_filtered_fasta_bis | \\\n",
        "        awk 'BEGIN {FS = \"[>;=\\t]\"} {print $2, $4, length($NF)}' | \\\n",
        "        sort --key=3,3n --key=1,1d --key=2,2n | \\\n",
        "        uniq --check-chars=${length_of_sequence_IDs} > \"${SAMPLE}.qual\" &\n",
        "}\n",
        "\n",
        "dereplicate_fasta() {\n",
        "    # dereplicate and discard expected error values (ee)\n",
        "    ${VSEARCH} \\\n",
        "        --derep_fulllength fifo_filtered_fasta \\\n",
        "        --sizeout \\\n",
        "        --fasta_width 0 \\\n",
        "        --xee \\\n",
        "        --output \"${SAMPLE}.fas\" 2>> \"${SAMPLE}.log\"\n",
        "}\n",
        "\n",
        "list_local_clusters() {\n",
        "    # retain only clusters with more than 2 reads\n",
        "    # (do not use the fastidious option here)\n",
        "    ${SWARM} \\\n",
        "        --threads \"${THREADS}\" \\\n",
        "        --differences 1 \\\n",
        "        --usearch-abundance \\\n",
        "        --log /dev/null \\\n",
        "        --output-file /dev/null \\\n",
        "        --statistics-file - \\\n",
        "        \"${SAMPLE}.fas\" | \\\n",
        "        awk 'BEGIN {FS = OFS = \"\\t\"} $2 > 2' > \"${SAMPLE}.stats\"\n",
        "}\n",
        "\n",
        "## ------------------------------------------------------------------------ main\n",
        "# from raw fastq files to ready-to-use sample files\n",
        "declare -r ANTI_PRIMER_R=\"$(revcomp \"${PRIMER_R}\")\"\n",
        "find . -name \"${FASTQ_NAME_PATTERN}\" -type f -print0 | \\\n",
        "    while IFS= read -r -d '' FORWARD ; do\n",
        "        TICKER=$(( $TICKER + 1 ))\n",
        "        echo -e \"${TICKER}\\t${FORWARD}\"\n",
        "        REVERSE=\"${FORWARD/_1\\./_2.}\"  # adapt to fastq name patterns\n",
        "        SAMPLE=\"${FORWARD/_1_1.*/}\"\n",
        "\n",
        "        # clean (remove older files, if any)\n",
        "        rm --force \"${SAMPLE}\".{fas,qual,log,stats} ${FIFOS}\n",
        "        mkfifo ${FIFOS}\n",
        "\n",
        "        merge_fastq_pair\n",
        "        trim_primers\n",
        "        convert_fastq_to_fasta\n",
        "        extract_expected_error_values\n",
        "        dereplicate_fasta\n",
        "        list_local_clusters\n",
        "\n",
        "        # clean (make sure fifos are not reused)\n",
        "        rm ${FIFOS}\n",
        "        unset FORWARD REVERSE SAMPLE\n",
        "    done"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baH-W-SwYud4",
      "metadata": {
        "id": "baH-W-SwYud4"
      },
      "source": [
        "To adapt this code to another dataset, you just need to change the\n",
        "primer sequences in the initial block of variables, and the raw fastq\n",
        "file search pattern and sample file naming (in the final `while` loop),\n",
        "if your raw fastq files follow another naming rule.\n",
        "\n",
        "Let’s have a look at the data folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scMdsZA0Yud4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scMdsZA0Yud4",
        "outputId": "b019a1a2-0b25-4786-d972-689850f7f051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B010_1_1.fastq.gz  B050_1_2.fastq.gz  B090.fas\t\t L030.log\t    L070.qual\n",
            "B010_1_2.fastq.gz  B050.fas\t      B090.log\t\t L030.qual\t    L070.stats\n",
            "B010.fas\t   B050.log\t      B090.qual\t\t L030.stats\t    L080_1_1.fastq.gz\n",
            "B010.log\t   B050.qual\t      B090.stats\t L040_1_1.fastq.gz  L080_1_2.fastq.gz\n",
            "B010.qual\t   B050.stats\t      B100_1_1.fastq.gz  L040_1_2.fastq.gz  L080.fas\n",
            "B010.stats\t   B060_1_1.fastq.gz  B100_1_2.fastq.gz  L040.fas\t    L080.log\n",
            "B020_1_1.fastq.gz  B060_1_2.fastq.gz  B100.fas\t\t L040.log\t    L080.qual\n",
            "B020_1_2.fastq.gz  B060.fas\t      B100.log\t\t L040.qual\t    L080.stats\n",
            "B020.fas\t   B060.log\t      B100.qual\t\t L040.stats\t    L090_1_1.fastq.gz\n",
            "B020.log\t   B060.qual\t      B100.stats\t L050_1_1.fastq.gz  L090_1_2.fastq.gz\n",
            "B020.qual\t   B060.stats\t      L010_1_1.fastq.gz  L050_1_2.fastq.gz  L090.fas\n",
            "B020.stats\t   B070_1_1.fastq.gz  L010_1_2.fastq.gz  L050.fas\t    L090.log\n",
            "B030_1_1.fastq.gz  B070_1_2.fastq.gz  L010.fas\t\t L050.log\t    L090.qual\n",
            "B030_1_2.fastq.gz  B070.fas\t      L010.log\t\t L050.qual\t    L090.stats\n",
            "B030.fas\t   B070.log\t      L010.qual\t\t L050.stats\t    L100_1_1.fastq.gz\n",
            "B030.log\t   B070.qual\t      L010.stats\t L060_1_1.fastq.gz  L100_1_2.fastq.gz\n",
            "B030.qual\t   B070.stats\t      L020_1_1.fastq.gz  L060_1_2.fastq.gz  L100.fas\n",
            "B030.stats\t   B080_1_1.fastq.gz  L020_1_2.fastq.gz  L060.fas\t    L100.log\n",
            "B040_1_1.fastq.gz  B080_1_2.fastq.gz  L020.fas\t\t L060.log\t    L100.qual\n",
            "B040_1_2.fastq.gz  B080.fas\t      L020.log\t\t L060.qual\t    L100.stats\n",
            "B040.fas\t   B080.log\t      L020.qual\t\t L060.stats\t    MD5SUM\n",
            "B040.log\t   B080.qual\t      L020.stats\t L070_1_1.fastq.gz\n",
            "B040.qual\t   B080.stats\t      L030_1_1.fastq.gz  L070_1_2.fastq.gz\n",
            "B040.stats\t   B090_1_1.fastq.gz  L030_1_2.fastq.gz  L070.fas\n",
            "B050_1_1.fastq.gz  B090_1_2.fastq.gz  L030.fas\t\t L070.log\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./data/\n",
        "\n",
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "450TNoLXYud4",
      "metadata": {
        "id": "450TNoLXYud4"
      },
      "source": [
        "### variable declarations\n",
        "\n",
        "``` shell\n",
        "VAR1=\"some text\"\n",
        "declare -r VAR1=\"more text\"\n",
        "declare -ri VAR2=42\n",
        "```\n",
        "\n",
        "Here, we are using `declare -r` to indicate that `VAR1` is a constant.\n",
        "If we try to modify it somewhere in the code, that’s a bug, and the\n",
        "execution will stop with an error message.\n",
        "\n",
        "`declare -ri` indicates that `VAR2` is a constant integer.\n",
        "\n",
        "### R1 and R2 merging\n",
        "\n",
        "``` shell\n",
        "merge_fastq_pair() {\n",
        "    ${VSEARCH} \\\n",
        "        --threads \"${THREADS}\" \\\n",
        "        --fastq_mergepairs \"${FORWARD}\" \\\n",
        "        --reverse \"${REVERSE}\" \\\n",
        "        --fastq_ascii \"${ENCODING}\" \\\n",
        "        --fastq_allowmergestagger \\\n",
        "        --fastqout fifo_merged_fastq 2> \"${SAMPLE}.log\" &\n",
        "}\n",
        "```\n",
        "\n",
        "before merging:\n",
        "\n",
        "    R1: ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequen\n",
        "                                                   ||||\n",
        "                                                   quence-ESREVER_REMIRP-GAT-ROTPADA :R2\n",
        "\n",
        "after merging:\n",
        "\n",
        "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
        "\n",
        "A minimal overlap and similarity is required for the merging. Reads that\n",
        "can’t be merged are discarded. `vsearch` re-computes the quality values\n",
        "of the overlapping positions. An overlap corresponds to a double\n",
        "observation, and the error probability must then be re-evaluated for\n",
        "each position in the overlap. Some mergers do it the wrong way.\n",
        "\n",
        "![fastq\\_merging\\_theory](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/fastq_merging_theory.png)\n",
        "\n",
        "Note: according to [Edgar and Flyvberg\n",
        "(2015)](https://doi.org/10.1093/bioinformatics/btv401) merging is an\n",
        "important step that can change radically the apparent diversity profile\n",
        "of a community (some popular mergers do not cope well with variable\n",
        "length markers).\n",
        "\n",
        "### reverse-complement\n",
        "\n",
        "``` shell\n",
        "revcomp() {\n",
        "    # reverse-complement a DNA/RNA IUPAC string\n",
        "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
        "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
        "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
        "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
        "}\n",
        "```\n",
        "\n",
        "That function takes a primer sequence, for example `ACTTTCGTTCTTGATYRA`\n",
        "and outputs the reverse-complement of that sequence\n",
        "(`TYRATCAAGAACGAAAGT`). This is useful when searching for primers after\n",
        "merging, because the reverse primer is reverse-complemented in merged\n",
        "reads:\n",
        "\n",
        "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
        "\n",
        "### trim primers\n",
        "\n",
        "After merging, we use cutadapt to trim primers:\n",
        "\n",
        "``` shell\n",
        "trim_primers() {\n",
        "    # search forward primer in both normal and revcomp: now all reads\n",
        "    # are in the same orientation\n",
        "    ${CUTADAPT} \\\n",
        "        --revcomp \\\n",
        "        --front \"${PRIMER_F};rightmost\" \\\n",
        "        --overlap \"${MIN_F}\" fifo_merged_fastq 2>> \"${SAMPLE}.log\" | \\\n",
        "        ${CUTADAPT} \\\n",
        "            --adapter \"${ANTI_PRIMER_R}\" \\\n",
        "            --overlap \"${MIN_R}\" \\\n",
        "            --max-n 0 - > fifo_trimmed_fastq 2>> \"${SAMPLE}.log\" &\n",
        "}\n",
        "```\n",
        "\n",
        "Before:\n",
        "\n",
        "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
        "\n",
        "the first call removes the *rightmost* forward primer (and everything\n",
        "before):\n",
        "\n",
        "                               actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
        "\n",
        "the second call removes the reverse primer (and everything after).\n",
        "\n",
        "                               actual_target_sequence\n",
        "\n",
        "At either step, if a primer is not found, the read is discarded.\n",
        "\n",
        "`--revcomp`: some sequencing protocols can produce reads in random\n",
        "orientations (R1 contains a mix of forward and reverse reads). That\n",
        "option has the effect of re-orienting the reads.\n",
        "\n",
        "`--overlap`: by default, cutadapt considers that an overlap of three\n",
        "nucleotides is enough for a match. Here we require a minimal overlap\n",
        "equal to 2/3rd of the length of our primers.\n",
        "\n",
        "    Match:\n",
        "\n",
        "       MER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
        "       |||||||||||\n",
        "    PRIMER_FORWARD\n",
        "\n",
        "\n",
        "    No Match\n",
        "              WARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
        "              ||||\n",
        "    PRIMER_FORWARD\n",
        "\n",
        "Note: with `--max-n 0`, reads with uncertain nucleotides (`N`) are\n",
        "discarded.\n",
        "\n",
        "### convert fastq to fasta\n",
        "\n",
        "A simple format conversion:\n",
        "\n",
        "``` shell\n",
        "convert_fastq_to_fasta() {\n",
        "    # use SHA1 values as sequence names,\n",
        "    # compute expected error values (ee)\n",
        "    ${VSEARCH} \\\n",
        "        --fastq_filter fifo_trimmed_fastq \\\n",
        "        --relabel_sha1 \\\n",
        "        --fastq_ascii \"${ENCODING}\" \\\n",
        "        --eeout \\\n",
        "        --fasta_width 0 \\\n",
        "        --fastaout - 2>> \"${SAMPLE}.log\" | \\\n",
        "        tee fifo_filtered_fasta_bis > fifo_filtered_fasta &\n",
        "}\n",
        "```\n",
        "\n",
        "For example, a fastq entry such as:\n",
        "\n",
        "    @s1\n",
        "    AAAA\n",
        "    +\n",
        "    IIII\n",
        "\n",
        "will be transformed into a `fasta` entry, with a sequence-derived name\n",
        "(hash value of the sequence), and the computed expected error (later\n",
        "used for quality filtering):\n",
        "\n",
        "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
        "    AAAA\n",
        "\n",
        "Why using hash values as sequence names?\n",
        "\n",
        "-   easy to compute,\n",
        "-   compact fix length name,\n",
        "-   readable (you get use to it),\n",
        "-   a given sequence will always produce the same hash,\n",
        "-   allows cross-studies comparisons\n",
        "\n",
        "### extract expected error values\n",
        "\n",
        "Extract and store read lengths and expected error values (later used for\n",
        "quality filtering). If two reads have the exact same sequence, keep the\n",
        "best (lowest) expected error.\n",
        "\n",
        "``` shell\n",
        "extract_expected_error_values() {\n",
        "    # extract ee for future quality filtering (keep the lowest\n",
        "    # observed expected error value for each unique sequence)\n",
        "    local -ri length_of_sequence_IDs=40\n",
        "    paste - - < fifo_filtered_fasta_bis | \\\n",
        "        awk 'BEGIN {FS = \"[>;=\\t]\"} {print $2, $4, length($NF)}' | \\\n",
        "        sort --key=3,3n --key=1,1d --key=2,2n | \\\n",
        "        uniq --check-chars=${length_of_sequence_IDs} > \"${SAMPLE}.qual\" &\n",
        "}\n",
        "```\n",
        "\n",
        "The result is a file with three columns: 1. amplicon name, 1. lowest\n",
        "expected error observed for that amplicon in that sample, 1. amplicon\n",
        "length\n",
        "\n",
        "    17765e625e2e3588a21306012dfd8f162b8944b8 0.003813 48\n",
        "    3dd5e4500bd663e4dd27bf0e2ae5b41cb9eee2b9 0.004051 51\n",
        "    138408e37cf3c86e025cf6d5f6828eff9dbd1015 0.004448 56\n",
        "    75f075940a92869137394c5a0b9d06d5c4aad2f6 0.5106 57\n",
        "    1a8baf082848f50d2019ed162c22e87e02122868 0.004766 60\n",
        "    3b6f64c93290cee1f1102065ddcb345355e0f223 0.005084 64\n",
        "    d0b25e99d48fd996dedb179cefae6941c768abb5 0.005322 67\n",
        "    319f8eb5171101539d3fb7255c3edd500ec5eb6b 0.005481 69\n",
        "    28d992d5a8900d23be60dee0e07f27872a30f770 0.03710 70\n",
        "    c7c55f0c514ce8c2a5ee49f02523dadcd96a4109 0.01171 78\n",
        "    ...\n",
        "\n",
        "Note: that function could be simplified now that `vsearch` version 2.23\n",
        "can add length attributes `;length=123` to fastq and fasta headers.\n",
        "\n",
        "### dereplicate\n",
        "\n",
        "The first significant lossless reduction of our dataset! In\n",
        "metabarcoding datasets, some sequences are rare and only observed once,\n",
        "whereas some sequences are present in many exact copies. When coded\n",
        "correctly, finding identical sequences is a very fast and efficient\n",
        "operation:\n",
        "\n",
        "``` shell\n",
        "dereplicate_fasta() {\n",
        "    # dereplicate and discard expected error values (ee)\n",
        "    ${VSEARCH} \\\n",
        "        --derep_fulllength fifo_filtered_fasta \\\n",
        "        --sizeout \\\n",
        "        --fasta_width 0 \\\n",
        "        --xee \\\n",
        "        --output \"${SAMPLE}.fas\" 2>> \"${SAMPLE}.log\"\n",
        "}\n",
        "```\n",
        "\n",
        "For instance, this fasta file:\n",
        "\n",
        "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
        "    AAAA\n",
        "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
        "    AAAA\n",
        "\n",
        "will be dereplicated into this one:\n",
        "\n",
        "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=2\n",
        "    AAAA\n",
        "\n",
        "Note that expected error values are removed by the `-xee` option, and\n",
        "that a new attribute `;size=2` has been added to represent the fact that\n",
        "this particular sequence has been observed twice.\n",
        "\n",
        "### list local clusters\n",
        "\n",
        "Later in this analysis, we will need to search for cluster\n",
        "co-occurrences on a per-sample basis. For each sample, we quickly\n",
        "generate clusters and a list of cluster seeds with `swarm` and store the\n",
        "results. Here we use `swarm` for the first time, we will give more\n",
        "details when `swarm` will be used to process the whole dataset (pooled\n",
        "samples), in the second part of the pipeline.\n",
        "\n",
        "``` shell\n",
        "list_local_clusters() {\n",
        "    # retain only clusters with more than 2 reads\n",
        "    # (do not use the fastidious option here)\n",
        "    ${SWARM} \\\n",
        "        --threads \"${THREADS}\" \\\n",
        "        --differences 1 \\\n",
        "        --usearch-abundance \\\n",
        "        --log /dev/null \\\n",
        "        --output-file /dev/null \\\n",
        "        --statistics-file - \\\n",
        "        \"${SAMPLE}.fas\" | \\\n",
        "        awk 'BEGIN {FS = OFS = \"\\t\"} $2 > 2' > \"${SAMPLE}.stats\"\n",
        "}\n",
        "```\n",
        "\n",
        "Here, `swarm` reads a fasta file, representing a sample, and produces a\n",
        "table looking like that:\n",
        "\n",
        "| uniq | abundance | seed                                     | abundance | singletons | layers | steps |\n",
        "|------|-----------|------------------------------------------|-----------|------------|--------|-------|\n",
        "| 5897 | 22010     | 3db68d2f77252793f23d7089d0d4103eb8942dcb | 3612      | 4600       | 8      | 8     |\n",
        "| 5528 | 15548     | 45211af6b7811bf45b5d3694054b800b5b13efd4 | 3728      | 4488       | 9      | 9     |\n",
        "| 3058 | 13452     | 47e639615ad19c8dededae45f38beb52c4e9861d | 3542      | 2252       | 9      | 9     |\n",
        "| 2172 | 8268      | 307ab3d7f513adc74854dd72e817fe02f7601db2 | 3722      | 1639       | 8      | 8     |\n",
        "| 1453 | 6754      | e52a67c43758a5f9a39b4cb42ac10cf41958e1d4 | 3563      | 1053       | 4      | 4     |\n",
        "| 1437 | 8836      | 962e2976f63ec09db8361ce4a498f50ade4b1674 | 3712      | 1046       | 4      | 4     |\n",
        "| 1303 | 6307      | 79ae1f05f177ad948d18dc86a7944772c63273b9 | 3559      | 971        | 6      | 6     |\n",
        "| 1126 | 6032      | b0835c4fba3d39e25059371902b0774741e3a57d | 3562      | 748        | 5      | 5     |\n",
        "| 759  | 5355      | 91be8585fcad245e9f3a53578207c58355426583 | 3603      | 540        | 4      | 4     |\n",
        "| 743  | 5981      | 42871cf2b4deaa78cbd1c163847567685c56d44e | 3654      | 567        | 4      | 4     |\n",
        "\n",
        "Note: clusters with only two or less reads are discarded (not useful for\n",
        "*cleaving*).\n",
        "\n",
        "In the second part of the pipeline, we are going to use that table as a\n",
        "list of sequences that played the role of cluster seeds in this\n",
        "particular sample. Don’t worry, that part will be explained in details.\n",
        "\n",
        "Note: the number of layers or steps does not reflect the real pairwise\n",
        "distance (pairwise distance is usually smaller):\n",
        "\n",
        "![step vs pairwise\n",
        "distance](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/steps_vs_pairwise_distance_example.png)\n",
        "\n",
        "### loop over each pair of fastq files\n",
        "\n",
        "Finally, we search for all fastq R1 files (`find`) and we apply our\n",
        "functions (merge, trim, convert, extract, dereplicate, clusterize). If\n",
        "we remove the visual clutter, it looks like this:\n",
        "\n",
        "``` shell\n",
        "find . -name \"${FASTQ_NAME_PATTERN}\" -type f -print0 | \\\n",
        "    while IFS= read -r -d '' FORWARD ; do\n",
        "        ...\n",
        "        merge_fastq_pair\n",
        "        trim_primers\n",
        "        convert_fastq_to_fasta\n",
        "        extract_expected_error_values\n",
        "        dereplicate_fasta\n",
        "        list_local_clusters\n",
        "        ...\n",
        "    done\n",
        "```\n",
        "\n",
        "That’s it for the first part of the pipeline!\n",
        "\n",
        "The whole process is very fast, even for large datasets. It is also easy\n",
        "to distribute the computation load on many machines, if need be.\n",
        "\n",
        "### trimming and merging success rate?\n",
        "\n",
        "Read merging and primer trimming are lossy filters (reads are lost). It\n",
        "is a good practice to control the per-sample and overall yield:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qus5yDZWYud5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qus5yDZWYud5",
        "outputId": "be1ed7c9-ecca-46f3-8bc0-0d1285beafa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B010\t3359\t2109\t1929\t1878\t\n",
            "B020\t6987\t4191\t3722\t3647\t\n",
            "B030\t5130\t3371\t3089\t2993\t\n",
            "B040\t7158\t4502\t4035\t3969\t\n",
            "B050\t2903\t1931\t1716\t1341\t\n",
            "B060\t7494\t4337\t3716\t3693\t\n",
            "B070\t1034\t720\t611\t418\t\n",
            "B080\t8614\t5703\t5328\t5226\t\n",
            "B090\t9268\t6005\t5704\t5632\t\n",
            "B100\t11644\t7301\t6774\t6740\t\n",
            "L010\t6484\t4101\t3721\t3588\t\n",
            "L020\t3656\t2392\t2195\t2066\t\n",
            "L030\t3372\t2084\t1889\t1767\t\n",
            "L040\t7981\t5127\t4711\t4655\t\n",
            "L050\t8092\t5377\t5061\t4997\t\n",
            "L060\t7789\t5167\t4707\t4616\t\n",
            "L070\t7215\t4777\t4353\t4293\t\n",
            "L080\t3577\t2207\t1996\t1917\t\n",
            "L090\t7210\t4291\t3991\t3914\t\n",
            "L100\t8502\t5470\t4949\t4874\t\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./data/\n",
        "\n",
        "# format as a table\n",
        "for f in *.log ; do\n",
        "    echo -en \"${f/\\.log/}\\t\"\n",
        "    awk '{if (NR == 1) {printf \"%s\\t\", $1}\n",
        "          if (NR == 2) {printf $1\"\\t\"}}' ${f}\n",
        "    grep \"Reads with adapters\" \"${f}\" | \\\n",
        "        awk '{printf $4\"\\t\"}' | tr -d \",\"\n",
        "      printf \"\\n\"\n",
        "  done"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L2BQQWRyYud5",
      "metadata": {
        "id": "L2BQQWRyYud5"
      },
      "source": [
        "| samples | reads | assembled | F    | R    |\n",
        "|---------|-------|-----------|------|------|\n",
        "| B010    | 3359  | 2109      | 1928 | 1877 |\n",
        "| B020    | 6987  | 4191      | 3721 | 3646 |\n",
        "| B030    | 5130  | 3371      | 3089 | 2993 |\n",
        "| B040    | 7158  | 4502      | 4033 | 3967 |\n",
        "| B050    | 2903  | 1931      | 1716 | 1341 |\n",
        "| B060    | 7494  | 4337      | 3716 | 3693 |\n",
        "| B070    | 1034  | 720       | 611  | 418  |\n",
        "| B080    | 8614  | 5703      | 5327 | 5225 |\n",
        "| B090    | 9268  | 6005      | 5702 | 5630 |\n",
        "| B100    | 11644 | 7301      | 6774 | 6740 |\n",
        "| L010    | 6484  | 4101      | 3721 | 3588 |\n",
        "| L020    | 3656  | 2392      | 2195 | 2065 |\n",
        "| L030    | 3372  | 2084      | 1888 | 1766 |\n",
        "| L040    | 7981  | 5127      | 4710 | 4654 |\n",
        "| L050    | 8092  | 5377      | 5060 | 4996 |\n",
        "| L060    | 7789  | 5167      | 4707 | 4616 |\n",
        "| L070    | 7215  | 4777      | 4352 | 4292 |\n",
        "| L080    | 3577  | 2207      | 1996 | 1917 |\n",
        "| L090    | 7210  | 4291      | 3991 | 3914 |\n",
        "| L100    | 8502  | 5470      | 4947 | 4872 |\n",
        "\n",
        "A yield above 80% is good. Below 60%, something might be wrong with your\n",
        "run or your biological material (bad chemistry, target sequence too\n",
        "long, non-specific amplification, wrong primer sequences, etc.).\n",
        "\n",
        "Part 1¾: taxonomic references\n",
        "-----------------------------\n",
        "\n",
        "Before tackling the second part of the pipeline, we need to prepare our\n",
        "reference database that is going to be used for the taxonomic assignment\n",
        "of our environmental sequences.\n",
        "\n",
        "We are working with 18S V4 amplicons. We could use\n",
        "[Silva](http://www.arb-silva.de/) SSU, but\n",
        "[PR2](https://github.com/pr2database/pr2database), the Protist Ribosomal\n",
        "Reference database, is a well-curated and eukaryote specific database of\n",
        "SSU (18S) references. Let's use the latest version (5.1), published\n",
        "earlier this month:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nuWqerL3Yud5",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuWqerL3Yud5",
        "outputId": "8db56c1d-3e43-4c32-d4d9-7ee3e90ec293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-05 21:16:52--  https://github.com/pr2database/pr2database/releases/download/v5.1.0.0/pr2_version_5.1.0_SSU_UTAX.fasta.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/105268099/90ee28da-3898-45ea-b8fb-7caed65092e5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250405%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250405T211652Z&X-Amz-Expires=300&X-Amz-Signature=e9b2909ff67238c8dbd68a70364606130025af7e9c91e70d2776b84c630d4495&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dpr2_version_5.1.0_SSU_UTAX.fasta.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-04-05 21:16:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/105268099/90ee28da-3898-45ea-b8fb-7caed65092e5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250405%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250405T211652Z&X-Amz-Expires=300&X-Amz-Signature=e9b2909ff67238c8dbd68a70364606130025af7e9c91e70d2776b84c630d4495&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dpr2_version_5.1.0_SSU_UTAX.fasta.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58439627 (56M) [application/octet-stream]\n",
            "Saving to: ‘pr2_version_5.1.0_SSU_UTAX.fasta.gz’\n",
            "\n",
            "pr2_version_5.1.0_S 100%[===================>]  55.73M  36.1MB/s    in 1.5s    \n",
            "\n",
            "2025-04-05 21:16:54 (36.1 MB/s) - ‘pr2_version_5.1.0_SSU_UTAX.fasta.gz’ saved [58439627/58439627]\n",
            "\n",
            "\u001b[32m\u001b[K21363\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K>AF203895.1.1766_U;tax=k:Eukaryota,d:Archaeplastida,p:Rhodophyta-Eurhodophytina,c:Florideophyceae,o:Ceramiales,f:Rhodomelaceae,g:Epiglossum,s:Epiglossum_proliferum\u001b[01;31m\u001b[K \u001b[m\u001b[K\n",
            "\u001b[32m\u001b[K402327\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K>MF093915.1.1767_U;tax=k:Eukaryota,d:Archaeplastida,p:Rhodophyta-Eurhodophytina,c:Florideophyceae,o:Ceramiales,f:Rhodomelaceae,g:Alsidium,s:Alsidium_seaforthii\u001b[01;31m\u001b[K \u001b[m\u001b[K\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./references/\n",
        "\n",
        "revcomp() {\n",
        "    # reverse-complement a DNA/RNA IUPAC string\n",
        "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
        "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
        "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
        "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
        "}\n",
        "\n",
        "## download PR2 (UTAX version)\n",
        "wget https://github.com/pr2database/pr2database/releases/download/v5.1.0.0/pr2_version_5.1.0_SSU_UTAX.fasta.gz\n",
        "declare -r SOURCE=\"pr2_version_5.1.0_SSU_UTAX.fasta\"\n",
        "## search for non-ASCII characters\n",
        "zgrep --color='auto' -P -n '[^\\x00-\\x7F]' \"${SOURCE}.gz\"\n",
        "\n",
        "## extract the V4 region (primers from Stoeck et al. 2010)\n",
        "declare -r PRIMER_F=\"CCAGCASCYGCGGTAATTCC\"\n",
        "declare -r PRIMER_R=\"ACTTTCGTTCTTGATYRA\"\n",
        "declare -r ANTI_PRIMER_R=\"$(revcomp \"${PRIMER_R}\")\"\n",
        "declare -r OUTPUT=\"${SOURCE/_UTAX*/}_${PRIMER_F}_${PRIMER_R}.fas\"\n",
        "declare -r LOG=\"${OUTPUT/.fas/.log}\"\n",
        "declare -r MIN_LENGTH=\"32\"\n",
        "declare -r ERROR_RATE=\"0.2\"\n",
        "declare -r MIN_F=$(( ${#PRIMER_F} * 1 / 3 ))\n",
        "declare -r MIN_R=$(( ${#PRIMER_R} * 1 / 3 ))\n",
        "declare -r OPTIONS=\"--minimum-length ${MIN_LENGTH} --discard-untrimmed --error-rate ${ERROR_RATE}\"\n",
        "declare -r CUTADAPT=\"$(which cutadapt) ${OPTIONS}\"\n",
        "\n",
        "zcat \"${SOURCE}.gz\" | \\\n",
        "    dos2unix | \\\n",
        "    sed '/^>/ s/;tax=k:/ /\n",
        "         /^>/ s/,[dpcofgs]:/|/g\n",
        "         /^>/ ! s/U/T/g' | \\\n",
        "     ${CUTADAPT} \\\n",
        "         --revcomp \\\n",
        "         --front \"${PRIMER_F}\" \\\n",
        "         --overlap \"${MIN_F}\" - 2> \"${LOG}\" | \\\n",
        "         ${CUTADAPT} \\\n",
        "         --adapter \"${ANTI_PRIMER_R}\" \\\n",
        "         --overlap \"${MIN_R}\" - > \"${OUTPUT}\" 2>> \"${LOG}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GidTGo3XYud5",
      "metadata": {
        "id": "GidTGo3XYud5"
      },
      "source": [
        "I won’t go into details here, but you might notice some familiar code\n",
        "such as `revcomp()` and two calls to `cutadapt`. The goal here is to\n",
        "download and trim the reference sequences:\n",
        "\n",
        "    reference 1: xxxxxxxx-PRIMER_F-target_region-PRIMER_R-xxxxx\n",
        "    reference 2:            IMER_F-target_region-PRIMER_R-xxxxxxxxxxx\n",
        "    reference 3:                    arget-region-PRIMER_R-xxxxxxxxxxxxxxxxxxxx\n",
        "\n",
        "Discard references that do not contain our target region (flanked by our\n",
        "primers), and trim the primers. The reference dataset is now much\n",
        "smaller, and limited to our target region:\n",
        "\n",
        "    reference 1:                   target_region\n",
        "    reference 2:                   target_region\n",
        "\n",
        "Note: while preparing this, I’ve found a small bug in the latest PR2\n",
        "release ([weird character in one species\n",
        "name](https://github.com/pr2database/pr2database/issues/37))\n",
        "\n",
        "### subsample PR2\n",
        "\n",
        "With a complete PR2 reference dataset, taxonomic assignment would take\n",
        "more than an hour on a Google colab instance (that’s the most\n",
        "computationally intensive step in whole the pipeline). To speed up\n",
        "things, I provide a PR2 subset with exactly what we need to assign the\n",
        "Neotropical dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YXXKDSWQYud5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXXKDSWQYud5",
        "outputId": "1a5e9eb6-038f-47c4-8805-089092d0211a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-03 21:20:33--  https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/references/useful_references.list\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/references/useful_references.list [following]\n",
            "--2025-04-03 21:20:33--  https://raw.githubusercontent.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/main/references/useful_references.list\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./references/\n",
        "\n",
        "URL=\"https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/references\"\n",
        "LIST=\"useful_references.list\"\n",
        "TRIMMED_PR2=\"pr2_version_5.1.0_SSU_CCAGCASCYGCGGTAATTCC_ACTTTCGTTCTTGATYRA.fas\"\n",
        "\n",
        "wget --continue \"${URL}/${LIST}\"\n",
        "\n",
        "grep \\\n",
        "    --no-group-separator \\\n",
        "    --after-context=1 \\\n",
        "    --fixed-strings \\\n",
        "    --file \"${LIST}\" \\\n",
        "    \"${TRIMMED_PR2}\" > \"${TRIMMED_PR2/\\.fas/_subset.fas}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ZE7bMsJYud5",
      "metadata": {
        "id": "1ZE7bMsJYud5"
      },
      "source": [
        "Part 2: from fasta files to an annotated occurrence table\n",
        "---------------------------------------------------------\n",
        "\n",
        "![pipeline\n",
        "overview](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/diapo_pipeline_final_colour.png)\n",
        "\n",
        "The pipeline is divided into two parts. A first part where each sample\n",
        "is processed individually. And a second part where all samples are\n",
        "pooled to produce an occurrence table.\n",
        "\n",
        "In this second part of the pipeline, we will:\n",
        "\n",
        "1.  pool metadata (amplicon distributions, local clustering results),\n",
        "2.  pool fasta samples and dereplicate,\n",
        "3.  clusterize the whole dataset,\n",
        "4.  detect chimeras,\n",
        "5.  *cleave* (separate un-correlated sub-clusters; complement to\n",
        "    `lulu`),\n",
        "6.  assign to taxonomic references,\n",
        "7.  build a filtered occurrence table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EG8iWEPrYud5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG8iWEPrYud5",
        "outputId": "ccb7a6a4-e6be-4c21-b840-860ed1a6fbe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run global clustering and chimera detection...\n",
            "\n",
            "run cleaving...\n",
            "PROGRESS: parsing per-sample stats\n",
            "PROGRESS: parsing stats\n",
            "PROGRESS: parsing swarms\n",
            "PROGRESS: parsing struct\n",
            "PROGRESS: sorting each cluster\n",
            "PROGRESS: computing per-cluster stats\n",
            "PROGRESS: computing per-cluster swarms\n",
            "PROGRESS: parsing fasta file\n",
            "\n",
            "build first OTU table...\n",
            "PROGRESS: parsing taxonomic assignments\n",
            "PROGRESS: parsing fasta representatives\n",
            "PROGRESS: parsing stats\n",
            "PROGRESS: parsing swarms\n",
            "PROGRESS: parsing uchime\n",
            "PROGRESS: parsing amplicon quality (EE)\n",
            "PROGRESS: parsing distribution file\n",
            "PROGRESS: filtering and writing OTUs\n",
            "\n",
            "taxonomic assignment...\n",
            "\n",
            "build final OTU table...\n",
            "PROGRESS: parsing taxonomy\n",
            "PROGRESS: parsing and updating old OTU table\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "cd ./results/\n",
        "export LC_ALL=C\n",
        "\n",
        "## ------------------------------------------------------------ define variables\n",
        "declare -r PROJECT=\"Neotropical_soils_18S_V4\"\n",
        "declare -r DATA_FOLDER=\"../data/\"\n",
        "declare -r SWARM=\"$(which swarm)\"  # swarm 3.0 or more recent\n",
        "declare -r VSEARCH=\"$(which vsearch)\"  # vsearch 2.21 or more recent\n",
        "declare -ri THREADS=2\n",
        "declare -ri RESOLUTION=1\n",
        "declare -ri FILTER=2\n",
        "declare -r SRC=\"../src\"\n",
        "declare -r OTU_CLEAVER=\"OTU_cleaver.py\"\n",
        "declare -r OTU_TABLE_BUILDER=\"OTU_contingency_table_filtered.py\"\n",
        "declare -r OTU_TABLE_UPDATER=\"OTU_table_updater.py\"\n",
        "declare -r STAMPA_MERGE=\"stampa_merge.py\"\n",
        "declare -r DATABASE=\"../references/pr2_version_5.1.0_SSU_CCAGCASCYGCGGTAATTCC_ACTTTCGTTCTTGATYRA_subset.fas\"\n",
        "\n",
        "N_SAMPLES=$(find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
        "                -type f ! -empty -print0 | tr -d -c '\\0' | wc --chars)\n",
        "FINAL_FASTA=\"${PROJECT}_${N_SAMPLES}_samples.fas\"\n",
        "QUALITY_FILE=\"${FINAL_FASTA%.*}.qual\"\n",
        "DISTRIBUTION_FILE=\"${FINAL_FASTA%.*}.distr\"\n",
        "POTENTIAL_SUB_SEEDS=\"${FINAL_FASTA%.*}_per_sample_OTUs.stats\"\n",
        "LOG=\"${FINAL_FASTA%.*}.log\"\n",
        "OUTPUT_SWARMS=\"${FINAL_FASTA%.*}_${RESOLUTION}f.swarms\"\n",
        "OUTPUT_LOG=\"${FINAL_FASTA%.*}_${RESOLUTION}f.log\"\n",
        "OUTPUT_STATS=\"${FINAL_FASTA%.*}_${RESOLUTION}f.stats\"\n",
        "OUTPUT_STRUCT=\"${FINAL_FASTA%.*}_${RESOLUTION}f.struct\"\n",
        "OUTPUT_REPRESENTATIVES=\"${FINAL_FASTA%.*}_${RESOLUTION}f_representatives.fas\"\n",
        "TAXONOMIC_ASSIGNMENTS=\"${OUTPUT_REPRESENTATIVES%.*}.results\"\n",
        "UCHIME_RESULTS=\"${OUTPUT_REPRESENTATIVES%.*}.uchime\"\n",
        "UCHIME_LOG=\"${OUTPUT_REPRESENTATIVES%.*}.log\"\n",
        "OTU_TABLE=\"${FINAL_FASTA%.*}.OTU.filtered.cleaved.table\"\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------- global clustering\n",
        "echo \"run global clustering and chimera detection...\"\n",
        "\n",
        "## Build expected error file\n",
        "find \"${DATA_FOLDER}\" -name \"*.qual\" \\\n",
        "    -type f ! -empty -print0 | \\\n",
        "    sort -k3,3n -k1,1d -k2,2n --merge --files0-from=- | \\\n",
        "    uniq --check-chars=40 > \"${QUALITY_FILE}\" &\n",
        "\n",
        "\n",
        "## Build distribution file (sequence <-> sample relations)\n",
        "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
        "    -type f ! -empty -execdir grep -H \"^>\" '{}' \\; | \\\n",
        "    sed 's/.*\\/// ; s/\\.fas:>/\\t/ ; s/;size=/\\t/ ; s/;$//' | \\\n",
        "    awk 'BEGIN {FS = OFS = \"\\t\"} {print $2, $1, $3}' > \"${DISTRIBUTION_FILE}\" &\n",
        "\n",
        "\n",
        "## list all cluster seeds of size > 2\n",
        "find \"${DATA_FOLDER}\" -name \"*.stats\" \\\n",
        "    -type f ! -empty -execdir grep --with-filename \"\" '{}' \\; | \\\n",
        "    sed 's/^\\.\\/// ; s/\\.stats:/\\t/' > \"${POTENTIAL_SUB_SEEDS}\" &\n",
        "\n",
        "\n",
        "## global dereplication\n",
        "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
        "    -type f ! -empty -execdir cat '{}' + | \\\n",
        "    \"${VSEARCH}\" \\\n",
        "        --derep_fulllength - \\\n",
        "        --quiet \\\n",
        "        --sizein \\\n",
        "        --sizeout \\\n",
        "        --log \"${LOG}\" \\\n",
        "        --fasta_width 0 \\\n",
        "        --output \"${FINAL_FASTA}\"\n",
        "\n",
        "\n",
        "## clustering (swarm 3 or more recent)\n",
        "\"${SWARM}\" \\\n",
        "    --differences \"${RESOLUTION}\" \\\n",
        "    --fastidious \\\n",
        "    --usearch-abundance \\\n",
        "    --threads \"${THREADS}\" \\\n",
        "    --internal-structure \"${OUTPUT_STRUCT}\" \\\n",
        "    --output-file \"${OUTPUT_SWARMS}\" \\\n",
        "    --statistics-file \"${OUTPUT_STATS}\" \\\n",
        "    --seeds \"${OUTPUT_REPRESENTATIVES}\" \\\n",
        "    \"${FINAL_FASTA}\" 2> \"${OUTPUT_LOG}\"\n",
        "\n",
        "\n",
        "## fake taxonomic assignment\n",
        "grep \"^>\" \"${OUTPUT_REPRESENTATIVES}\" | \\\n",
        "    sed --regexp-extended 's/^>//\n",
        "                           s/;size=/\\t/\n",
        "                           s/;?$/\\t0.0\\tNA\\tNA/' > \"${TAXONOMIC_ASSIGNMENTS}\"\n",
        "\n",
        "\n",
        "## chimera detection\n",
        "## discard sequences with an abundance lower than FILTER\n",
        "## and search for chimeras\n",
        "\"${VSEARCH}\" \\\n",
        "    --fastx_filter \"${OUTPUT_REPRESENTATIVES}\"  \\\n",
        "    --quiet \\\n",
        "    --minsize \"${FILTER}\" \\\n",
        "    --fastaout - | \\\n",
        "    \"${VSEARCH}\" \\\n",
        "        --uchime_denovo - \\\n",
        "        --quiet \\\n",
        "        --uchimeout \"${UCHIME_RESULTS}\" \\\n",
        "        2> \"${UCHIME_LOG}\"\n",
        "\n",
        "\n",
        "## ------------------------------------------------------------------- cleaving\n",
        "echo\n",
        "echo \"run cleaving...\"\n",
        "\n",
        "## split OTUs\n",
        "python3 \\\n",
        "    \"${SRC}/${OTU_CLEAVER}\" \\\n",
        "    --global_stats \"${OUTPUT_STATS}\" \\\n",
        "    --per_sample_stats \"${POTENTIAL_SUB_SEEDS}\" \\\n",
        "    --struct \"${OUTPUT_STRUCT}\" \\\n",
        "    --swarms \"${OUTPUT_SWARMS}\" \\\n",
        "    --fasta \"${FINAL_FASTA}\"\n",
        "\n",
        "## fake taxonomic assignment\n",
        "grep \"^>\" \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
        "    sed --regexp-extended 's/^>//\n",
        "                           s/;size=/\\t/\n",
        "                           s/;?$/\\t0.0\\tNA\\tNA/' > \"${TAXONOMIC_ASSIGNMENTS}2\"\n",
        "\n",
        "## chimera detection (only down to the smallest newly cleaved OTU)\n",
        "LOWEST_ABUNDANCE=$(sed --regexp-extended --quiet \\\n",
        "    '/^>/ s/.*;size=([0-9]+);?/\\1/p' \\\n",
        "    \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
        "    sort --numeric-sort | \\\n",
        "    head -n 1)\n",
        "\n",
        "# sort and filter by abundance (default to an abundance of 1), search\n",
        "# for chimeras\n",
        "cat \"${OUTPUT_REPRESENTATIVES}\" \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
        "    \"${VSEARCH}\" \\\n",
        "        --sortbysize - \\\n",
        "        --quiet \\\n",
        "        --sizein \\\n",
        "        --minsize ${LOWEST_ABUNDANCE:-1} \\\n",
        "        --sizeout \\\n",
        "        --output - | \\\n",
        "        \"${VSEARCH}\" \\\n",
        "            --uchime_denovo - \\\n",
        "            --quiet \\\n",
        "            --uchimeout \"${UCHIME_RESULTS}2\" \\\n",
        "            2> \"${OUTPUT_REPRESENTATIVES%.*}.log2\"\n",
        "\n",
        "unset LOWEST_ABUNDANCE\n",
        "\n",
        "\n",
        "## ------------------------------------------------------------ first OTU table\n",
        "echo\n",
        "echo \"build first OTU table...\"\n",
        "\n",
        "# build OTU table\n",
        "python3 \\\n",
        "    \"${SRC}/${OTU_TABLE_BUILDER}\" \\\n",
        "    --representatives <(cat \"${OUTPUT_REPRESENTATIVES}\"{,2}) \\\n",
        "    --stats <(cat \"${OUTPUT_STATS}\"{,2}) \\\n",
        "    --swarms <(cat \"${OUTPUT_SWARMS}\"{,2}) \\\n",
        "    --chimera <(cat \"${UCHIME_RESULTS}\"{,2}) \\\n",
        "    --quality \"${QUALITY_FILE}\" \\\n",
        "    --assignments <(cat \"${TAXONOMIC_ASSIGNMENTS}\"{2,}) \\\n",
        "    --distribution \"${DISTRIBUTION_FILE}\" > \"${OTU_TABLE}\"\n",
        "\n",
        "\n",
        "# extract fasta sequences from OTU table\n",
        "awk 'NR > 1 {printf \">\"$4\";size=\"$2\";\\n\"$10\"\\n\"}' \"${OTU_TABLE}\" \\\n",
        "    > \"${OTU_TABLE/.table/.fas}\"\n",
        "\n",
        "\n",
        "\n",
        "## ------------------------------------------------------- taxonomic assignment\n",
        "echo\n",
        "echo \"taxonomic assignment...\"\n",
        "\n",
        "# search for best hits\n",
        "${VSEARCH} \\\n",
        "    --usearch_global \"${OTU_TABLE/.table/.fas}\" \\\n",
        "    --db ${DATABASE} \\\n",
        "    --quiet \\\n",
        "    --threads ${THREADS} \\\n",
        "    --dbmask none \\\n",
        "    --qmask none \\\n",
        "    --rowlen 0 \\\n",
        "    --notrunclabels \\\n",
        "    --userfields query+id1+target \\\n",
        "    --maxaccepts 0 \\\n",
        "    --maxrejects 0 \\\n",
        "    --top_hits_only \\\n",
        "    --output_no_hits \\\n",
        "    --id 0.5 \\\n",
        "    --iddef 1 \\\n",
        "    --userout - | \\\n",
        "    sed 's/;size=/_/ ; s/;//' > hits.representatives\n",
        "\n",
        "# in case of multi-best hit, find the last-common ancestor\n",
        "python3 ${SRC}/${STAMPA_MERGE} $(pwd)\n",
        "\n",
        "# sort by decreasing abundance\n",
        "sort -k2,2nr -k1,1d results.representatives > \"${OTU_TABLE/.table/.results}\"\n",
        "\n",
        "# clean-up\n",
        "rm hits.representatives results.representatives\n",
        "\n",
        "\n",
        "## ----------------------------------------------------------- update OTU table\n",
        "echo\n",
        "echo \"build final OTU table...\"\n",
        "NEW_TABLE=$(mktemp)\n",
        "\n",
        "python3 \\\n",
        "    \"${SRC}/${OTU_TABLE_UPDATER}\" \\\n",
        "    --old_otu_table \"${OTU_TABLE}\" \\\n",
        "    --new_taxonomy \"${OTU_TABLE/.table/.results}\" \\\n",
        "    --new_otu_table \"${NEW_TABLE}\"\n",
        "\n",
        "# fix OTU sorting\n",
        "(head -n 1 \"${NEW_TABLE}\"\n",
        "    tail -n +2 \"${NEW_TABLE}\" | \\\n",
        "    sort -k2,2nr | \\\n",
        "    nl --number-format='ln' --number-width=1 | \\\n",
        "    cut --complement -f 2\n",
        ") > \"${OTU_TABLE}2\"\n",
        "\n",
        "# clean up\n",
        "rm \"${NEW_TABLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SsbBwnBVhBc6",
      "metadata": {
        "id": "SsbBwnBVhBc6"
      },
      "outputs": [],
      "source": [
        "ls ./results/  -alh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_21p5WAgYud5",
      "metadata": {
        "id": "_21p5WAgYud5"
      },
      "source": [
        "### pool expected error values\n",
        "\n",
        "So far we had an expected error value for each unique amplicon in each\n",
        "sample. Here, we group them to get an expected error value for each\n",
        "unique amplicon in the bioproject:\n",
        "\n",
        "``` shell\n",
        "### Build expected error file\n",
        "find \"${DATA_FOLDER}\" -name \"*.qual\" \\\n",
        "    -type f ! -empty -print0 | \\\n",
        "    sort -k3,3n -k1,1d -k2,2n --merge --files0-from=- | \\\n",
        "    uniq --check-chars=40 > \"${QUALITY_FILE}\" &\n",
        "```\n",
        "\n",
        "The result is a table with three-columns, sorted by increasing length:\n",
        "\n",
        "1.  amplicon name,\n",
        "2.  lowest expected error observed for that amplicon,\n",
        "3.  amplicon length\n",
        "\n",
        "<!-- -->\n",
        "\n",
        "    964ab48de96fa2997896c4d80db52e412d5da64d 0.002542 32\n",
        "    f55456130e6d1a43af8dd89ee58432273d872775 0.002542 32\n",
        "    4fbd2164d6898278c5981f3a18701d9b44e18a5a 0.002621 33\n",
        "    88885c6241c729d020f8fffc4f7505bb599e5bf8 0.002621 33\n",
        "    c4505959d778ac5eb94f4971fe902f9c34c48763 0.002701 34\n",
        "    dca48a272d66c555b06317af2472455db8140237 0.002780 35\n",
        "    9f2cd2223ed79292d3317fc50b0060bb1a726296 0.002939 37\n",
        "    7ceba3f3dee8aaaeac5079b5ee0364e38bf5006e 0.3192 38\n",
        "    86127aa0eb63c3a047705c09095b2b12a88856d6 0.004934 38\n",
        "    94b7001f660573b59e33fea8e53da0afb51d9f69 0.007367 38\n",
        "    ...\n",
        "\n",
        "### distribution\n",
        "\n",
        "For each unique amplicon in the dataset, record the samples where it\n",
        "occurs, and its abundance in that sample (number of reads):\n",
        "\n",
        "``` shell\n",
        "## Build distribution file (sequence <-> sample relations)\n",
        "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
        "    -type f ! -empty -execdir grep -H \"^>\" '{}' \\; | \\\n",
        "    sed 's/.*\\/// ; s/\\.fas:>/\\t/ ; s/;size=/\\t/ ; s/;$//' | \\\n",
        "    awk 'BEGIN {FS = OFS = \"\\t\"} {print $2, $1, $3}' > \"${DISTRIBUTION_FILE}\" &\n",
        "```\n",
        "\n",
        "The results is a three-column table:\n",
        "\n",
        "``` text\n",
        "f414cfec8c1cc3973e95e67cff46299a00e8368a    S001    7369\n",
        "16b79e33e7897ca08ecaa282dc4b4ba1e6d6b460    S001    679\n",
        "ea5b349a8215a8b8ca2de29f0a33087b7c7d5e77    S001    458\n",
        "1f51f06217fa2ac348b50fea587702e29bfe1f1c    S001    315\n",
        "288366fe126296cb6c6aec488dde3b25a6385d5f    S001    243\n",
        "f59d28a1e80d582e07d0b224f07ac6e360a8acef    S001    154\n",
        "e74f16e7aaeaae903b09ddb6d9f16c66acd47f9a    S001    139\n",
        "be8e83ba27fa599c1eccb416e06131870ba101e6    S001    119\n",
        "324f9ad3b7651dbbca36abd62ed9c39238db12c1    S001    98\n",
        "2ba9bb0fe66318a974db3a580b23b3b1691f4a54    S001    87\n",
        "```\n",
        "\n",
        "### local cluster representatives\n",
        "\n",
        "Earlier in this analysis, we used `swarm` to make a list of clusters and\n",
        "cluster seeds present in each sample. Here, we are simply pooling these\n",
        "per-sample lists into a single file, adding the sample name:\n",
        "\n",
        "``` shell\n",
        "## list all cluster seeds of size > 2\n",
        "find \"${DATA_FOLDER}\" -name \"*.stats\" \\\n",
        "    -type f ! -empty -execdir grep --with-filename \"\" '{}' \\; | \\\n",
        "    sed 's/^\\.\\/// ; s/\\.stats:/\\t/' > \"${POTENTIAL_SUB_SEEDS}\" &\n",
        "```\n",
        "\n",
        "This information will be used during the cleaving step:\n",
        "\n",
        "| sample | uniq | abundance | seed                                     | abundance | singletons | layers | steps |\n",
        "|--------|------|-----------|------------------------------------------|-----------|------------|--------|-------|\n",
        "| L010   | 5897 | 22010     | 3db68d2f77252793f23d7089d0d4103eb8942dcb | 3612      | 4600       | 8      | 8     |\n",
        "| L010   | 5528 | 15548     | 45211af6b7811bf45b5d3694054b800b5b13efd4 | 3728      | 4488       | 9      | 9     |\n",
        "| L010   | 3058 | 13452     | 47e639615ad19c8dededae45f38beb52c4e9861d | 3542      | 2252       | 9      | 9     |\n",
        "| L010   | 2172 | 8268      | 307ab3d7f513adc74854dd72e817fe02f7601db2 | 3722      | 1639       | 8      | 8     |\n",
        "| L010   | 1453 | 6754      | e52a67c43758a5f9a39b4cb42ac10cf41958e1d4 | 3563      | 1053       | 4      | 4     |\n",
        "| L010   | 1437 | 8836      | 962e2976f63ec09db8361ce4a498f50ade4b1674 | 3712      | 1046       | 4      | 4     |\n",
        "| L010   | 1303 | 6307      | 79ae1f05f177ad948d18dc86a7944772c63273b9 | 3559      | 971        | 6      | 6     |\n",
        "| L010   | 1126 | 6032      | b0835c4fba3d39e25059371902b0774741e3a57d | 3562      | 748        | 5      | 5     |\n",
        "| L010   | 759  | 5355      | 91be8585fcad245e9f3a53578207c58355426583 | 3603      | 540        | 4      | 4     |\n",
        "| L010   | 743  | 5981      | 42871cf2b4deaa78cbd1c163847567685c56d44e | 3654      | 567        | 4      | 4     |\n",
        "\n",
        "### pool fasta entries\n",
        "\n",
        "Metabarcoding samples often share some of their amplicons. Find all the\n",
        "non-empty fasta files created during the first part of the pipeline, and\n",
        "dereplicate them. This is a fast and lossless reduction of the volume of\n",
        "our data:\n",
        "\n",
        "``` shell\n",
        "## global dereplication\n",
        "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
        "    -type f ! -empty -execdir cat '{}' + | \\\n",
        "    \"${VSEARCH}\" \\\n",
        "        --derep_fulllength - \\\n",
        "        --quiet \\\n",
        "        --sizein \\\n",
        "        --sizeout \\\n",
        "        --log \"${LOG}\" \\\n",
        "        --fasta_width 0 \\\n",
        "        --output \"${FINAL_FASTA}\"\n",
        "```\n",
        "\n",
        "Note: this time we use the option `--sizein` to take into account the\n",
        "abundance obtained during sample-level dereplications.\n",
        "\n",
        "For instance, this fasta entries coming from different samples:\n",
        "\n",
        "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=3\n",
        "    AAAA\n",
        "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=2\n",
        "    AAAA\n",
        "\n",
        "will be dereplicated into this one:\n",
        "\n",
        "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=5\n",
        "    AAAA\n",
        "\n",
        "Dereplication is strongly recommended before clustering with `swarm`!\n",
        "\n",
        "### clustering\n",
        "\n",
        "Global clustering with `swarm`.\n",
        "\n",
        "Why do we need clustering/denoising?\n",
        "\n",
        "Amplification and sequencing are **noisy**, and the level of noise is\n",
        "directly correlated to the initial abundance of your target sequence:\n",
        "\n",
        "![seed vs\n",
        "cloud](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/seed_vs_cloud.png)\n",
        "\n",
        "The number of microvariants (errors?) around a seed (true sequence?) is\n",
        "a function of the abundance of the seed. Abundant seeds will be at the\n",
        "center of a dense and large cloud of microvariants.\n",
        "\n",
        "`swarm` will use that. With default parameters, it will generate all\n",
        "possible microvariants of an amplicon and simply check if these\n",
        "microvariants are present in the dataset and capture them if they are.\n",
        "Iteratively.\n",
        "\n",
        "I recommend to use a local difference of one (`d = 1`) with the\n",
        "`--fastidious` option. A local difference of one is `swarm`’s default\n",
        "and recommended setting. Combining it with the `--fastidious` option is\n",
        "a very good compromise between taxonomic resolution and molecular\n",
        "diversity inflation (too many clusters):\n",
        "\n",
        "``` shell\n",
        "## clustering (swarm 3 or more recent)\n",
        "\"${SWARM}\" \\\n",
        "    --differences \"${RESOLUTION}\" \\\n",
        "    --fastidious \\\n",
        "    --usearch-abundance \\\n",
        "    --threads \"${THREADS}\" \\\n",
        "    --internal-structure \"${OUTPUT_STRUCT}\" \\\n",
        "    --output-file \"${OUTPUT_SWARMS}\" \\\n",
        "    --statistics-file \"${OUTPUT_STATS}\" \\\n",
        "    --seeds \"${OUTPUT_REPRESENTATIVES}\" \\\n",
        "    \"${FINAL_FASTA}\" 2> \"${OUTPUT_LOG}\"\n",
        "```\n",
        "\n",
        "`swarm` outputs clusters and cluster representatives in fasta format.\n",
        "`swarm` can also output interesting stats and a description of the\n",
        "internal structure of the clusters. We are going to use all these files.\n",
        "\n",
        "`swarm`’s log contains a lot of information. The most important ones are\n",
        "the number of clusters, the number of unique sequences in the largest\n",
        "cluster, and the memory consumption. In its current acceptation, the\n",
        "term OTU designates 97%-based clusters. In that sense, `swarm` produces\n",
        "ASVs (down to a one-nucleotide resolution after the cleaving step).\n",
        "\n",
        "### Chimera detection\n",
        "\n",
        "Chimeras are very frequent in metabarcoding dataset. Their frequency\n",
        "increases with the number of PCR cycles, and it is important to detect\n",
        "and remove them. The dominant algorithm today is `uchime` (citation),\n",
        "and we are using its `vsearch` implementation.\n",
        "\n",
        "To reduce the computational load, we are working with cluster\n",
        "representatives, rather than working with the whole fasta dataset.\n",
        "Similarly, we do not check the chimeric status of clusters with only one\n",
        "read (*singletons*):\n",
        "\n",
        "``` shell\n",
        "## chimera detection\n",
        "## discard sequences with an abundance lower than FILTER\n",
        "## and search for chimeras\n",
        "\"${VSEARCH}\" \\\n",
        "    --fastx_filter \"${OUTPUT_REPRESENTATIVES}\"  \\\n",
        "    --quiet \\\n",
        "    --minsize \"${FILTER}\" \\\n",
        "    --fastaout - | \\\n",
        "    \"${VSEARCH}\" \\\n",
        "        --uchime_denovo - \\\n",
        "        --quiet \\\n",
        "        --uchimeout \"${UCHIME_RESULTS}\" \\\n",
        "        2> \"${UCHIME_LOG}\"\n",
        "```\n",
        "\n",
        "Here is `vsearch`’s report:\n",
        "\n",
        "    vsearch v2.22.1_linux_x86_64, 62.7GB RAM, 8 cores\n",
        "    https://github.com/torognes/vsearch\n",
        "\n",
        "    Reading file - 100%\n",
        "    773659 nt in 2091 seqs, min 34, max 503, avg 370\n",
        "    Masking 100%\n",
        "    Sorting by abundance 100%\n",
        "    Counting k-mers 100%\n",
        "    Detecting chimeras 100%\n",
        "    Found 399 (19.1%) chimeras, 1666 (79.7%) non-chimeras,\n",
        "    and 26 (1.2%) borderline sequences in 2091 unique sequences.\n",
        "    Taking abundance information into account, this corresponds to\n",
        "    3869 (6.1%) chimeras, 59815 (93.6%) non-chimeras,\n",
        "    and 221 (0.3%) borderline sequences in 63905 total sequences.\n",
        "\n",
        "Note: a high number of chimeras. This is usual for long markers such as\n",
        "18S V4. The data loss is small in terms of reads (6.4% in total).\n",
        "\n",
        "Chimera detection is complex and should be a priority research field.\n",
        "There is a new chimera detection algorithm in preparation for vsearch!\n",
        "\n",
        "### cleaving\n",
        "\n",
        "This is a procedure I use to post-process `swarm`’s clustering results,\n",
        "and to improve our taxonomic resolution down to one nucleotide, where\n",
        "necessary.\n",
        "\n",
        "Where `lulu` groups similar clusters that have the same distribution\n",
        "patterns (co-occur in the same samples), cleaving does the complement\n",
        "operation. Cleaving separates cluster sub-parts (similar sequences) that\n",
        "do not co-occur.\n",
        "\n",
        "For example, a cluster with two sub-parts A and B:\n",
        "\n",
        "| cluster | s1  | s2  | s3  | s4  | s5  | s6  |\n",
        "|---------|-----|-----|-----|-----|-----|-----|\n",
        "| A       | 9   | 7   | 9   | 0   | 0   | 0   |\n",
        "| B       | 0   | 0   | 0   | 7   | 8   | 9   |\n",
        "\n",
        "A and B do not co-occur, there is some ecological signal here. Cleaving\n",
        "will create two separated clusters A and B.\n",
        "\n",
        "![cleaved\n",
        "cluster](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/grasp_nodD_764_samples_1_OTU_6.png)\n",
        "\n",
        "Cleaving is a fast operation, even for extremely large datasets. No\n",
        "artificial diversity inflation, resolution only when necessary.\n",
        "\n",
        "Cleaving has a visible effect on abundance vs. cloud distributions:\n",
        "\n",
        "![cleaved\n",
        "cluster](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/grasp_nodD_764_samples_1f.stats.before_after_OTU_breaking.png)\n",
        "\n",
        "Some outliers (above the average) are separated into sub-components that\n",
        "are much closer to the average abundance vs. cloud ratio.\n",
        "\n",
        "Note : after cleaving, it is necessary to perform a new chimera check.\n",
        "\n",
        "### build filtered occurrence table\n",
        "\n",
        "Pool all the information produced above, and use a python script to\n",
        "build an occurrence table:\n",
        "\n",
        "``` shell\n",
        "# build OTU table\n",
        "python3 \\\n",
        "    \"${SRC}/${OTU_TABLE_BUILDER}\" \\\n",
        "    --representatives <(cat \"${OUTPUT_REPRESENTATIVES}\"{,2}) \\\n",
        "    --stats <(cat \"${OUTPUT_STATS}\"{,2}) \\\n",
        "    --swarms <(cat \"${OUTPUT_SWARMS}\"{,2}) \\\n",
        "    --chimera <(cat \"${UCHIME_RESULTS}\"{,2}) \\\n",
        "    --quality \"${QUALITY_FILE}\" \\\n",
        "    --assignments <(cat \"${TAXONOMIC_ASSIGNMENTS}\"{2,}) \\\n",
        "    --distribution \"${DISTRIBUTION_FILE}\" > \"${OTU_TABLE}\"\n",
        "```\n",
        "\n",
        "This python script will also filter out chimeras, clusters with a low\n",
        "quality seed, and clusters with a low abundance and seen in only one or\n",
        "two samples (using the following code):\n",
        "\n",
        "``` python\n",
        "        if (\n",
        "            chimera_status == \"N\"\n",
        "            and high_quality <= EE_threshold\n",
        "            and (abundance >= 3 or spread >= 2)\n",
        "        ):\n",
        "```\n",
        "\n",
        "Note: eliminating chimeras **before** taxonomic assignment is a matter\n",
        "open for discussion. If a chimera is 100% identical to a reference\n",
        "sequence, then it could be a false positive (not a chimera) or a sign\n",
        "that the matching reference should be investigated.\n",
        "\n",
        "### Taxonomy: last-common ancestor\n",
        "\n",
        "Brute-force approach! This is the slowest step in the pipeline, by far.\n",
        "It can easily be distributed on a large cluster of computers and be done\n",
        "under 20 minutes, even for a large dataset. This is the method I use in\n",
        "my own projects, but I admit it is not the most efficient, and other\n",
        "methods should be explored (naive Bayesian classifier, Markov model,\n",
        "etc.):\n",
        "\n",
        "``` shell\n",
        "# search for best hits\n",
        "${VSEARCH} \\\n",
        "    --usearch_global \"${OTU_TABLE/.table/.fas}\" \\\n",
        "    --db ${DATABASE} \\\n",
        "    --quiet \\\n",
        "    --threads ${THREADS} \\\n",
        "    --dbmask none \\\n",
        "    --qmask none \\\n",
        "    --rowlen 0 \\\n",
        "    --notrunclabels \\\n",
        "    --userfields query+id1+target \\\n",
        "    --maxaccepts 0 \\\n",
        "    --maxrejects 0 \\\n",
        "    --top_hits_only \\\n",
        "    --output_no_hits \\\n",
        "    --id 0.5 \\\n",
        "    --iddef 1 \\\n",
        "    --userout - | \\\n",
        "    sed 's/;size=/_/ ; s/;//' > hits.representatives\n",
        "```\n",
        "\n",
        "    1462274 nt in 4001 seqs, min 87, max 466, avg 365\n",
        "    Counting k-mers 100%\n",
        "    Creating k-mer index 100%\n",
        "    Searching 100%\n",
        "    Matching unique query sequences: 1272 of 1358 (93.67%)\n",
        "\n",
        "Results:\n",
        "\n",
        "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    GU319785.1.833_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|Cryptosporidium13|Cryptosporidium13_muris\n",
        "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    EU156446.1.816_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|Cryptosporidium15|Cryptosporidium15_sp.\n",
        "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    JQ413358.1.796_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|Cryptosporidium13|Cryptosporidium13_muris\n",
        "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    AB000647.1.1737_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
        "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    JQ698930.1.1731_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
        "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    U00974.1.1719_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
        "    ...\n",
        "\n",
        "The advantage of this method is the clarity of the results. Since both\n",
        "query and reference cover the same genomic region (in-between our\n",
        "forward and reverse primers), then we can use a simple, easy to\n",
        "interpret similarity definition.\n",
        "\n",
        "As you can see a given environmental amplicon can be assigned to more\n",
        "than one reference, and these references can point to different taxa. To\n",
        "reflect that uncertain taxonomic assignment, each environmental amplicon\n",
        "is assigned to the last-common ancestor of its closest references:\n",
        "\n",
        "``` shell\n",
        "# in case of multi-best hit, find the last-common ancestor\n",
        "python3 ${SRC}/${STAMPA_MERGE} $(pwd)\n",
        "```\n",
        "\n",
        "Resulting in:\n",
        "\n",
        "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    GU319785.1.833_U,EU156446.1.816_U,JQ413358.1.796_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|*|*\n",
        "\n",
        "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    AB000647.1.1737_U,JQ698930.1.1731_U,U00974.1.1719_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
        "    ...\n",
        "\n",
        "If an environmental sequence is equidistant to several reference\n",
        "sequences, the it is assigned to the last-common ancestor (part of the\n",
        "taxonomic path that is common) of the references. Conflicting\n",
        "assignments are replaced with stars (`*`).\n",
        "\n",
        "This method is great for finding errors in reference databases. For\n",
        "example, the third most-abundant cluster in our dataset\n",
        "(`5fc4b348c56e446f6efdd9ee50d6388b8f691915`, with a total of 3,125\n",
        "reads) is assigned to `*|*|*|*|*|*|*|*` with a similarity of 100%.\n",
        "\n",
        "That’s weird, let’s have a look.\n",
        "\n",
        "This cluster has a 100% similarity with 244 reference sequences:\n",
        "\n",
        "-   242 references are from an Embryophyceae (*Zea mays*),\n",
        "-   2 references are from an unknown Embryophyceae (Embryophyceae XXX\n",
        "    sp.),\n",
        "-   1 reference is from a Bacteria (*Pseudooceanicola lipolyticus*, an\n",
        "    Alphaproteobacteria)\n",
        "\n",
        "This last entry is very likely a misassigned reference sequence, and our\n",
        "environmental sequence is probably an Embryophyceae (but not maize, as\n",
        "it was collected deep into the equatorial forest).\n",
        "\n",
        "### occurrence table\n",
        "\n",
        "#### structure\n",
        "\n",
        "1.  OTU number\n",
        "2.  total number of reads\n",
        "3.  cloud (total number of unique sequences)\n",
        "4.  amplicon (identifier of the OTU representative)\n",
        "5.  length (length of the OTU representative)\n",
        "6.  abundance (abundance of the OTU representative)\n",
        "7.  chimera (is it a chimera? Yes, No, ?)\n",
        "8.  spread (number of samples where the OTU occurs)\n",
        "9.  quality (minimum expected error observed for the OTU representative,\n",
        "    divided by sequence length)\n",
        "10. sequence (sequence of the OTU representative)\n",
        "11. identity (maximum similarity of the OTU representative with\n",
        "    reference sequences)\n",
        "12. taxonomy (taxonomic assignment of the OTU representative)\n",
        "13. references (reference sequences closest to the OTU representative)\n",
        "14. sample 1\n",
        "15. sample 2\n",
        "16. sample…\n",
        "\n",
        "#### first look at the table\n",
        "\n",
        "We seen earlier that our third most-abundant cluster was assigned to the\n",
        "last-common ancestor `*|*|*|*|*|*|*|*` with a similarity of 100%. That\n",
        "sequence is probably from a plant.\n",
        "\n",
        "Our most-abundant cluster is a Fungi (Ascomycota, with 96.8% of\n",
        "similarity), as is our fourth-most abundant cluster (Basidiomycota,\n",
        "99.7%). These Fungi groups are the two most frequent in soils.\n",
        "\n",
        "A striking feature is the presence of these abundant and weakly assigned\n",
        "clusters (5 out of the 10 most-abundant clusters, 70 to 80% similarity\n",
        "with references). These clusters represent a yet-to-be-described\n",
        "molecular diversity of Apicomplexans:\n",
        "\n",
        "| Taxa                     | reads |\n",
        "|--------------------------|-------|\n",
        "| Alveolata-Apicomplexa    | 24951 |\n",
        "| Opisthokonta-Fungi       | 16508 |\n",
        "| Streptophyta             | 7417  |\n",
        "| Opisthokonta-Metazoa     | 3115  |\n",
        "| Rhizaria-Cercozoa        | 1676  |\n",
        "| Alveolata-Ciliophora     | 1463  |\n",
        "| Stramenopiles-Gyrista    | 587   |\n",
        "| Alveolata-Perkinsea      | 465   |\n",
        "| Alveolata-Dinoflagellata | 331   |\n",
        "\n",
        "#### additional filters?\n",
        "\n",
        "As mentioned above, some basic filters were applied to build this table.\n",
        "In part 1, only reads that could be merged were kept, as well as reads\n",
        "with both primers, reads without Ns, and reads longer than 32 nucleotide\n",
        "after primer trimming. In part 2, clusters with chimeric seed, clusters\n",
        "with a low quality seed, and clusters with a low abundance and seen in\n",
        "only one or two samples were discarded?\n",
        "\n",
        "It is possible to go further, and to discard clusters containing short\n",
        "(or too long) sequences, or clusters with a similarity to references\n",
        "below a certain threshold, or it is possible to use tools such as\n",
        "`lulu`.\n",
        "\n",
        "Conclusion\n",
        "----------\n",
        "\n",
        "Metabarcoding’s main challenge is noise. Bioinformatics tools can solve\n",
        "part of the problem, but robust experimental designs with replicates and\n",
        "controls are your safest bet."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}